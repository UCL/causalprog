{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#causalprog","title":"causalprog","text":"<p>A Python package for causal modelling and inference with stochastic causal programming</p> <p>This project is developed in collaboration with the Centre for Advanced Research Computing, University College London.</p>"},{"location":"#about","title":"About","text":""},{"location":"#project-team","title":"Project team","text":"<ul> <li>Ricardo Silva (rbas-ucl)</li> <li>Jialin Yu (jialin-yu)</li> <li>Will Graham (willGraham01)</li> <li>Matthew Scroggs (mscroggs)</li> <li>Matt Graham (matt-graham)</li> </ul>"},{"location":"#research-software-engineering-contact","title":"Research software engineering contact","text":"<p>Centre for Advanced Research Computing, University College London (arc.collaborations@ucl.ac.uk)</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p><code>causalprog</code> requires Python 3.11\u20133.13.</p>"},{"location":"#installation","title":"Installation","text":"<p>We recommend installing in a project specific virtual environment. To install the latest development version of <code>causalprog</code> using <code>pip</code> in the currently active environment run</p> <pre><code>pip install git+https://github.com/UCL/causalprog.git\n</code></pre> <p>Alternatively create a local clone of the repository with</p> <pre><code>git clone https://github.com/UCL/causalprog.git\n</code></pre> <p>and then install in editable mode by running</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"#running-tests","title":"Running tests","text":"<p>Tests can be run across all compatible Python versions in isolated environments using <code>tox</code> by running</p> <pre><code>tox\n</code></pre> <p>To run tests manually in a Python environment with <code>pytest</code> installed run</p> <pre><code>pytest tests\n</code></pre> <p>again from the root of the repository.</p> <p>For more information about the testing suite, please see the documentation page.</p>"},{"location":"#building-documentation","title":"Building documentation","text":"<p>The MkDocs HTML documentation can be built locally by running</p> <pre><code>tox -e docs\n</code></pre> <p>from the root of the repository. The built documentation will be written to <code>site</code>.</p> <p>Alternatively to build and preview the documentation locally, in a Python environment with the optional <code>docs</code> dependencies installed, run</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work was funded by Engineering and Physical Sciences Research Council (EPSRC).</p>"},{"location":"LICENSE/","title":"License","text":""},{"location":"LICENSE/#mit-license","title":"MIT License","text":"<p>Copyright (c) 2025 University College London</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/","title":"API reference","text":"<p>causalprog package.</p>"},{"location":"api/#causalprog.algorithms","title":"<code>algorithms</code>","text":"<p>Algorithms.</p>"},{"location":"api/#causalprog.algorithms.do","title":"<code>do</code>","text":"<p>Algorithms for applying do to a graph.</p>"},{"location":"api/#causalprog.algorithms.do.do","title":"<code>do(graph, node, value, label=None)</code>","text":"<p>Apply do to a graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The graph to apply do to. This will be copied.</p> required <code>node</code> <code>str</code> <p>The label of the node to apply do to.</p> required <code>value</code> <code>float</code> <p>The value to set the node to.</p> required <code>label</code> <code>str | None</code> <p>The label of the new graph</p> <code>None</code> <p>Returns:</p> Type Description <code>Graph</code> <p>A copy of the graph with do applied</p> Source code in <code>src/causalprog/algorithms/do.py</code> <pre><code>def do(graph: Graph, node: str, value: float, label: str | None = None) -&gt; Graph:\n    \"\"\"\n    Apply do to a graph.\n\n    Args:\n        graph: The graph to apply do to. This will be copied.\n        node: The label of the node to apply do to.\n        value: The value to set the node to.\n        label: The label of the new graph\n\n    Returns:\n        A copy of the graph with do applied\n\n    \"\"\"\n    if label is None:\n        label = f\"{graph.label}|do({node}={value})\"\n\n    nodes = {n.label: deepcopy(n) for n in graph.nodes if n.label != node}\n\n    # Search through the old graph, identifying nodes that had parameters which were\n    # defined by the node being fixed in the DO operation.\n    # We recreate these nodes, but replace each such parameter we encounter with\n    # a constant parameter equal that takes the fixed value given as an input.\n    for n in nodes.values():\n        params = tuple(n.parameters.keys())\n        for parameter_name in params:\n            if n.parameters[parameter_name] == node:\n                # Swap the parameter to a constant parameter, giving it the fixed value\n                n.constant_parameters[parameter_name] = value\n                # Remove the parameter from the node's record of non-constant parameters\n                n.parameters.pop(parameter_name)\n\n    # Recursively remove nodes that are predecessors of removed nodes\n    nodes_to_remove: tuple[str, ...] = (node,)\n    while len(nodes_to_remove) &gt; 0:\n        nodes_to_remove = removable_nodes(graph, nodes)\n        for n in removable_nodes(graph, nodes):\n            nodes.pop(n)\n\n    # Check for nodes that are predecessors of both a removed node and a remaining node\n    # and throw an error if one of these is found\n    for n in nodes:\n        _, excluded = get_included_excluded_successors(graph, nodes, n)\n        if len(excluded) &gt; 0:\n            msg = (\n                \"Node that is predecessor of node set by do and \"\n                f'nodes that are not removed found (\"{n}\")'\n            )\n            raise ValueError(msg)\n\n    g = Graph(label=f\"{label}|do[{node}={value}]\")\n    for n in nodes.values():\n        g.add_node(n)\n\n    # Any nodes whose counterparts connect to other nodes in the network need\n    # to mimic these links.\n    for edge in graph.edges:\n        if edge[0].label in nodes and edge[1].label in nodes:\n            g.add_edge(edge[0].label, edge[1].label)\n\n    return g\n</code></pre>"},{"location":"api/#causalprog.algorithms.do.get_included_excluded_successors","title":"<code>get_included_excluded_successors(graph, node_list, successors_of)</code>","text":"<p>Split successors of a node into nodes included and not included in a list.</p> <p>Split the successorts of a node into a list of nodes that are included in the input node list and a list of nodes that are not in the list.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The graph</p> required <code>node_list</code> <code>dict[str, Node]</code> <p>A dictionary of nodes, indexed by label</p> required <code>successors_of</code> <code>str</code> <p>The node to check the successors of</p> required <p>Returns:</p> Type Description <code>tuple[tuple[str, ...], tuple[str, ...]]</code> <p>Lists of included and excluded nodes</p> Source code in <code>src/causalprog/algorithms/do.py</code> <pre><code>def get_included_excluded_successors(\n    graph: Graph, node_list: dict[str, Node], successors_of: str\n) -&gt; tuple[tuple[str, ...], tuple[str, ...]]:\n    \"\"\"\n    Split successors of a node into nodes included and not included in a list.\n\n    Split the successorts of a node into a list of nodes that are included in\n    the input node list and a list of nodes that are not in the list.\n\n    Args:\n        graph: The graph\n        node_list: A dictionary of nodes, indexed by label\n        successors_of: The node to check the successors of\n\n    Returns:\n        Lists of included and excluded nodes\n\n    \"\"\"\n    included = []\n    excluded = []\n    for n in graph.successors[graph.get_node(successors_of)]:\n        if n.label in node_list:\n            included.append(n)\n        else:\n            excluded.append(n)\n    return tuple(included), tuple(excluded)\n</code></pre>"},{"location":"api/#causalprog.algorithms.do.removable_nodes","title":"<code>removable_nodes(graph, nodes)</code>","text":"<p>Generate list of nodes that can be removed from the graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The graph</p> required <code>nodes</code> <code>dict[str, Node]</code> <p>A dictionary of nodes, indexed by label</p> required <p>Returns:</p> Type Description <code>tuple[str, ...]</code> <p>List of labels of removable nodes</p> Source code in <code>src/causalprog/algorithms/do.py</code> <pre><code>def removable_nodes(graph: Graph, nodes: dict[str, Node]) -&gt; tuple[str, ...]:\n    \"\"\"\n    Generate list of nodes that can be removed from the graph.\n\n    Args:\n        graph: The graph\n        nodes: A dictionary of nodes, indexed by label\n\n    Returns:\n        List of labels of removable nodes\n\n    \"\"\"\n    removable: list[str] = []\n    for n in nodes:\n        included, excluded = get_included_excluded_successors(graph, nodes, n)\n        if len(excluded) &gt; 0 and len(included) == 0:\n            removable.append(n)\n    return tuple(removable)\n</code></pre>"},{"location":"api/#causalprog.algorithms.moments","title":"<code>moments</code>","text":"<p>Algorithms for estimating the expectation and standard deviation.</p>"},{"location":"api/#causalprog.algorithms.moments.expectation","title":"<code>expectation(graph, outcome_node_label, samples, *, parameter_values=None, rng_key)</code>","text":"<p>Estimate the expectation of (a random variable attached to) a node in a graph.</p> Source code in <code>src/causalprog/algorithms/moments.py</code> <pre><code>def expectation(\n    graph: Graph,\n    outcome_node_label: str,\n    samples: int,\n    *,\n    parameter_values: dict[str, float] | None = None,\n    rng_key: jax.Array,\n) -&gt; float:\n    \"\"\"Estimate the expectation of (a random variable attached to) a node in a graph.\"\"\"\n    return moment(\n        1,\n        graph,\n        outcome_node_label,\n        samples,\n        rng_key=rng_key,\n        parameter_values=parameter_values,\n    )\n</code></pre>"},{"location":"api/#causalprog.algorithms.moments.moment","title":"<code>moment(order, graph, outcome_node_label, samples, *, parameter_values=None, rng_key)</code>","text":"<p>Estimate a moment of (a random variable attached to) a node in a graph.</p> Source code in <code>src/causalprog/algorithms/moments.py</code> <pre><code>def moment(\n    order: int,\n    graph: Graph,\n    outcome_node_label: str,\n    samples: int,\n    *,\n    parameter_values: dict[str, float] | None = None,\n    rng_key: jax.Array,\n) -&gt; float:\n    \"\"\"Estimate a moment of (a random variable attached to) a node in a graph.\"\"\"\n    return (\n        sum(\n            sample(\n                graph,\n                outcome_node_label,\n                samples,\n                rng_key=rng_key,\n                parameter_values=parameter_values,\n            )\n            ** order\n        )\n        / samples\n    )\n</code></pre>"},{"location":"api/#causalprog.algorithms.moments.sample","title":"<code>sample(graph, outcome_node_label, samples, *, parameter_values=None, rng_key)</code>","text":"<p>Sample data from (a random variable attached to) a node in a graph.</p> Source code in <code>src/causalprog/algorithms/moments.py</code> <pre><code>def sample(\n    graph: Graph,\n    outcome_node_label: str,\n    samples: int,\n    *,\n    parameter_values: dict[str, float] | None = None,\n    rng_key: jax.Array,\n) -&gt; npt.NDArray[float]:\n    \"\"\"Sample data from (a random variable attached to) a node in a graph.\"\"\"\n    nodes = graph.roots_down_to_outcome(outcome_node_label)\n\n    values: dict[str, npt.NDArray[float]] = {}\n    keys = jax.random.split(rng_key, len(nodes))\n\n    for node, key in zip(nodes, keys, strict=False):\n        values[node.label] = node.sample(\n            parameter_values if parameter_values else {},\n            values,\n            samples,\n            rng_key=key,\n        )\n    return values[outcome_node_label]\n</code></pre>"},{"location":"api/#causalprog.algorithms.moments.standard_deviation","title":"<code>standard_deviation(graph, outcome_node_label, samples, *, parameter_values=None, rng_key, rng_key_first_moment=None)</code>","text":"<p>Estimate the standard deviation of (a RV attached to) a node in a graph.</p> Source code in <code>src/causalprog/algorithms/moments.py</code> <pre><code>def standard_deviation(\n    graph: Graph,\n    outcome_node_label: str,\n    samples: int,\n    *,\n    parameter_values: dict[str, float] | None = None,\n    rng_key: jax.Array,\n    rng_key_first_moment: jax.Array | None = None,\n) -&gt; float:\n    \"\"\"Estimate the standard deviation of (a RV attached to) a node in a graph.\"\"\"\n    return (\n        moment(\n            2,\n            graph,\n            outcome_node_label,\n            samples,\n            rng_key=rng_key,\n            parameter_values=parameter_values,\n        )\n        - moment(\n            1,\n            graph,\n            outcome_node_label,\n            samples,\n            rng_key=rng_key if rng_key_first_moment is None else rng_key_first_moment,\n            parameter_values=parameter_values,\n        )\n        ** 2\n    ) ** 0.5\n</code></pre>"},{"location":"api/#causalprog.backend","title":"<code>backend</code>","text":"<p>Helper functionality for incorporating different backends.</p>"},{"location":"api/#causalprog.causal_problem","title":"<code>causal_problem</code>","text":"<p>Classes for defining causal problems.</p>"},{"location":"api/#causalprog.causal_problem.CausalEstimand","title":"<code>CausalEstimand</code>","text":"<p>               Bases: <code>_CPComponent</code></p> <p>A Causal Estimand.</p> <p>The causal estimand is the function that we want to minimise (and maximise) as part of a causal problem. It should be a scalar-valued function of the random variables appearing in a graph.</p> Source code in <code>src/causalprog/causal_problem/components.py</code> <pre><code>class CausalEstimand(_CPComponent):\n    \"\"\"\n    A Causal Estimand.\n\n    The causal estimand is the function that we want to minimise (and maximise)\n    as part of a causal problem. It should be a scalar-valued function of the\n    random variables appearing in a graph.\n    \"\"\"\n</code></pre>"},{"location":"api/#causalprog.causal_problem.CausalProblem","title":"<code>CausalProblem</code>","text":"<p>Defines a causal problem.</p> Source code in <code>src/causalprog/causal_problem/causal_problem.py</code> <pre><code>class CausalProblem:\n    \"\"\"Defines a causal problem.\"\"\"\n\n    _underlying_graph: Graph\n    causal_estimand: CausalEstimand\n    constraints: list[Constraint]\n\n    @property\n    def _ordered_components(self) -&gt; list[_CPComponent]:\n        \"\"\"Internal ordering for components of the causal problem.\"\"\"\n        return [*self.constraints, self.causal_estimand]\n\n    def __init__(\n        self,\n        graph: Graph,\n        *constraints: Constraint,\n        causal_estimand: CausalEstimand,\n    ) -&gt; None:\n        \"\"\"Create a new causal problem.\"\"\"\n        self._underlying_graph = graph\n        self.causal_estimand = causal_estimand\n        self.constraints = list(constraints)\n\n    def _associate_models_to_components(\n        self, n_samples: int\n    ) -&gt; tuple[list[Predictive], list[int]]:\n        \"\"\"\n        Create models to be used by components of the problem.\n\n        Depending on how many constraints (and the causal estimand) require effect\n        handlers to wrap `self._underlying_graph.model`, we will need to create several\n        predictive models to sample from. However, we also want to minimise the number\n        of such models we have to make, in order to minimise the time we spend\n        actually computing samples.\n\n        As such, in this method we determine:\n        - How many models we will need to build, by grouping the constraints and the\n          causal estimand by the handlers they use.\n        - Build these models, returning them in a list called `models`.\n        - Build another list that maps the index of components in\n          `self._ordered_components` to the index of the model in `models` that they\n          use. The causal estimand is by convention the component at index -1 of this\n          returned list.\n\n        Args:\n            n_samples: Value to be passed to `numpyro.Predictive`'s `num_samples`\n                argument for each of the models that are constructed from the underlying\n                graph.\n\n        Returns:\n            list[Predictive]: List of Predictive models, whose elements contain all the\n                models needed by the components.\n            list[int]: Mapping of component indexes (as per `self_ordered_components`)\n                to the index of the model in the first return argument that the\n                component uses.\n\n        \"\"\"\n        models: list[Predictive] = []\n        grouped_component_indexes: list[list[int]] = []\n        for index, component in enumerate(self._ordered_components):\n            # Determine if this constraint uses the same handlers as those of any of\n            # the other sets.\n            belongs_to_existing_group = False\n            for group in grouped_component_indexes:\n                # Pull any element from the group to compare models to.\n                # Items in a group are known to have the same model, so we can just\n                # pull out the first one.\n                group_element = self._ordered_components[group[0]]\n                # Check if the current constraint can also use this model.\n                if component.can_use_same_model_as(group_element):\n                    group.append(index)\n                    belongs_to_existing_group = True\n                    break\n\n            # If the component does not fit into any existing group, create a new\n            # group for it. And add the model corresponding to the group to the\n            # list of models.\n            if not belongs_to_existing_group:\n                grouped_component_indexes.append([index])\n\n                models.append(\n                    Predictive(\n                        component.apply_effects(self._underlying_graph.model),\n                        num_samples=n_samples,\n                    )\n                )\n\n        # Now \"invert\" the grouping, creating a mapping that maps the index of a\n        # component to the (index of the) model it uses.\n        component_index_to_model_index = []\n        for index in range(len(self._ordered_components)):\n            for group_index, group in enumerate(grouped_component_indexes):\n                if index in group:\n                    component_index_to_model_index.append(group_index)\n                    break\n        # All indexes should belong to at least one group (worst case scenario,\n        # their own individual group). Thus, it is safe to do the above to create\n        # the mapping from component index -&gt; model (group) index.\n        return models, component_index_to_model_index\n\n    def lagrangian(\n        self, n_samples: int = 1000, *, maximum_problem: bool = False\n    ) -&gt; Callable[[dict[str, npt.ArrayLike], npt.ArrayLike, jax.Array], npt.ArrayLike]:\n        \"\"\"\n        Return a function that evaluates the Lagrangian of this `CausalProblem`.\n\n        Following the\n        [KKT theorem](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions),\n        given the causal estimand and the constraints we can assemble a Lagrangian and\n        seek its stationary points, to in turn identify minimisers of the constrained\n        optimisation problem that we started with.\n\n        The Lagrangian returned is a mathematical function of its first two arguments.\n        The first argument is the same dictionary of parameters that is passed to models\n        like `Graph.model`, and is the values the parameters (represented by the\n        `ParameterNode`s) are taking. The second argument is a 1D vector of Lagrange\n        multipliers, whose length is equal to the number of constraints.\n\n        The remaining argument of the Lagrangian is the PRNG Key that should be used\n        when drawing samples.\n\n        Note that our current implementation assumes there are no equality constraints\n        being imposed (in which case, we would need a 3-argument Lagrangian function).\n\n        Args:\n            n_samples: The number of random samples to be drawn when estimating the\n                value of functions of the RVs.\n            maximum_problem: If passed as `True`, assemble the Lagrangian for the\n                maximisation problem. Otherwise assemble that for the minimisation\n                problem (default behaviour).\n\n        Returns:\n            The Lagrangian, as a function of the model parameters, Lagrange multipliers,\n                and PRNG key.\n\n        \"\"\"\n        maximisation_prefactor = -1.0 if maximum_problem else 1.0\n\n        # Build association between self.constraints and the model-samples that each\n        # one needs to use. We do this here, since once it is constructed, it is\n        # fixed, and doesn't need to be done each time we call the Lagrangian.\n        models, component_to_index_mapping = self._associate_models_to_components(\n            n_samples\n        )\n\n        def _inner(\n            parameter_values: dict[str, npt.ArrayLike],\n            l_mult: jax.Array,\n            rng_key: jax.Array,\n        ) -&gt; npt.ArrayLike:\n            # Draw samples from all models\n            all_samples = tuple(\n                sample_model(model, rng_key, parameter_values) for model in models\n            )\n\n            value = maximisation_prefactor * self.causal_estimand(all_samples[-1])\n            # TODO: https://github.com/UCL/causalprog/issues/87\n            value += sum(\n                l_mult[i] * c(all_samples[component_to_index_mapping[i]])\n                for i, c in enumerate(self.constraints)\n            )\n            return value\n\n        return _inner\n</code></pre>"},{"location":"api/#causalprog.causal_problem.CausalProblem.__init__","title":"<code>__init__(graph, *constraints, causal_estimand)</code>","text":"<p>Create a new causal problem.</p> Source code in <code>src/causalprog/causal_problem/causal_problem.py</code> <pre><code>def __init__(\n    self,\n    graph: Graph,\n    *constraints: Constraint,\n    causal_estimand: CausalEstimand,\n) -&gt; None:\n    \"\"\"Create a new causal problem.\"\"\"\n    self._underlying_graph = graph\n    self.causal_estimand = causal_estimand\n    self.constraints = list(constraints)\n</code></pre>"},{"location":"api/#causalprog.causal_problem.CausalProblem.lagrangian","title":"<code>lagrangian(n_samples=1000, *, maximum_problem=False)</code>","text":"<p>Return a function that evaluates the Lagrangian of this <code>CausalProblem</code>.</p> <p>Following the KKT theorem, given the causal estimand and the constraints we can assemble a Lagrangian and seek its stationary points, to in turn identify minimisers of the constrained optimisation problem that we started with.</p> <p>The Lagrangian returned is a mathematical function of its first two arguments. The first argument is the same dictionary of parameters that is passed to models like <code>Graph.model</code>, and is the values the parameters (represented by the <code>ParameterNode</code>s) are taking. The second argument is a 1D vector of Lagrange multipliers, whose length is equal to the number of constraints.</p> <p>The remaining argument of the Lagrangian is the PRNG Key that should be used when drawing samples.</p> <p>Note that our current implementation assumes there are no equality constraints being imposed (in which case, we would need a 3-argument Lagrangian function).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of random samples to be drawn when estimating the value of functions of the RVs.</p> <code>1000</code> <code>maximum_problem</code> <code>bool</code> <p>If passed as <code>True</code>, assemble the Lagrangian for the maximisation problem. Otherwise assemble that for the minimisation problem (default behaviour).</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable[[dict[str, ArrayLike], ArrayLike, Array], ArrayLike]</code> <p>The Lagrangian, as a function of the model parameters, Lagrange multipliers, and PRNG key.</p> Source code in <code>src/causalprog/causal_problem/causal_problem.py</code> <pre><code>def lagrangian(\n    self, n_samples: int = 1000, *, maximum_problem: bool = False\n) -&gt; Callable[[dict[str, npt.ArrayLike], npt.ArrayLike, jax.Array], npt.ArrayLike]:\n    \"\"\"\n    Return a function that evaluates the Lagrangian of this `CausalProblem`.\n\n    Following the\n    [KKT theorem](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions),\n    given the causal estimand and the constraints we can assemble a Lagrangian and\n    seek its stationary points, to in turn identify minimisers of the constrained\n    optimisation problem that we started with.\n\n    The Lagrangian returned is a mathematical function of its first two arguments.\n    The first argument is the same dictionary of parameters that is passed to models\n    like `Graph.model`, and is the values the parameters (represented by the\n    `ParameterNode`s) are taking. The second argument is a 1D vector of Lagrange\n    multipliers, whose length is equal to the number of constraints.\n\n    The remaining argument of the Lagrangian is the PRNG Key that should be used\n    when drawing samples.\n\n    Note that our current implementation assumes there are no equality constraints\n    being imposed (in which case, we would need a 3-argument Lagrangian function).\n\n    Args:\n        n_samples: The number of random samples to be drawn when estimating the\n            value of functions of the RVs.\n        maximum_problem: If passed as `True`, assemble the Lagrangian for the\n            maximisation problem. Otherwise assemble that for the minimisation\n            problem (default behaviour).\n\n    Returns:\n        The Lagrangian, as a function of the model parameters, Lagrange multipliers,\n            and PRNG key.\n\n    \"\"\"\n    maximisation_prefactor = -1.0 if maximum_problem else 1.0\n\n    # Build association between self.constraints and the model-samples that each\n    # one needs to use. We do this here, since once it is constructed, it is\n    # fixed, and doesn't need to be done each time we call the Lagrangian.\n    models, component_to_index_mapping = self._associate_models_to_components(\n        n_samples\n    )\n\n    def _inner(\n        parameter_values: dict[str, npt.ArrayLike],\n        l_mult: jax.Array,\n        rng_key: jax.Array,\n    ) -&gt; npt.ArrayLike:\n        # Draw samples from all models\n        all_samples = tuple(\n            sample_model(model, rng_key, parameter_values) for model in models\n        )\n\n        value = maximisation_prefactor * self.causal_estimand(all_samples[-1])\n        # TODO: https://github.com/UCL/causalprog/issues/87\n        value += sum(\n            l_mult[i] * c(all_samples[component_to_index_mapping[i]])\n            for i, c in enumerate(self.constraints)\n        )\n        return value\n\n    return _inner\n</code></pre>"},{"location":"api/#causalprog.causal_problem.Constraint","title":"<code>Constraint</code>","text":"<p>               Bases: <code>_CPComponent</code></p> <p>A Constraint that forms part of a causal problem.</p> <p>Constraints of a causal problem are derived properties of RVs for which we have observed data. The causal estimand is minimised (or maximised) subject to the predicted values of the constraints being close to their observed values in the data.</p> <p>Adding a constraint \\(g(\\theta)\\) to a causal problem (where \\(\\theta\\) are the parameters of the causal problem) essentially imposes an additional constraint on the minimisation problem;</p> \\[ g(\\theta) - g_{\\text{data}} \\leq \\epsilon, \\] <p>where \\(g_{\\text{data}}\\) is the observed data value for the quantity \\(g\\), and \\(\\epsilon\\) is some tolerance.</p> Source code in <code>src/causalprog/causal_problem/components.py</code> <pre><code>class Constraint(_CPComponent):\n    r\"\"\"\n    A Constraint that forms part of a causal problem.\n\n    Constraints of a causal problem are derived properties of RVs for which we\n    have observed data. The causal estimand is minimised (or maximised) subject\n    to the predicted values of the constraints being close to their observed\n    values in the data.\n\n    Adding a constraint $g(\\theta)$ to a causal problem (where $\\theta$ are the\n    parameters of the causal problem) essentially imposes an additional\n    constraint on the minimisation problem;\n\n    $$ g(\\theta) - g_{\\text{data}} \\leq \\epsilon, $$\n\n    where $g_{\\text{data}}$ is the observed data value for the quantity $g$,\n    and $\\epsilon$ is some tolerance.\n    \"\"\"\n\n    data: npt.ArrayLike\n    tolerance: npt.ArrayLike\n    _outer_norm: Callable[[npt.ArrayLike], float]\n\n    def __init__(\n        self,\n        *effect_handlers: ModelMask,\n        model_quantity: Callable[..., npt.ArrayLike],\n        outer_norm: Callable[[npt.ArrayLike], float] | None = None,\n        data: npt.ArrayLike = 0.0,\n        tolerance: float = 1.0e-6,\n    ) -&gt; None:\n        r\"\"\"\n        Create a new constraint.\n\n        Constraints have the form\n\n        $$ c(\\theta) :=\n        \\mathrm{norm}\\left( g(\\theta)\n        - g_{\\mathrm{data}} \\right)\n        - \\epsilon $$\n\n        where;\n        - $\\mathrm{norm}$ is the outer norm of the constraint (`outer_norm`),\n        - $g(\\theta)$ is the model quantity involved in the constraint\n            (`model_quantity`),\n        - $g_{\\mathrm{data}}$ is the observed data (`data`),\n        - $\\epsilon$ is the tolerance in the data (`tolerance`).\n\n        In a causal problem, each constraint appears as the condition $c(\\theta)\\leq 0$\n        in the minimisation / maximisation (hence the inclusion of the $-\\epsilon$\n        term within $c(\\theta)$ itself).\n\n        $g$ should be a (possibly vector-valued) function that acts on (a subset of)\n        samples from the random variables of the causal problem. It must accept\n        variable keyword-arguments only, and should access the samples for each random\n        variable by indexing via the RV names (node labels). It should return the\n        model quantity as computed from the samples, that $g_{\\mathrm{data}}$ observed.\n\n        $g_{\\mathrm{data}}$ should be a fixed value whose shape is broadcast-able with\n        the return shape of $g$. It defaults to $0$ if not explicitly set.\n\n        $\\mathrm{norm}$ should be a suitable norm to take on the difference between the\n        model quantity as predicted by the samples ($g$) and the observed data\n        ($g_{\\mathrm{data}}$). It must return a scalar value. The default is the 2-norm.\n        \"\"\"\n        super().__init__(*effect_handlers, do_with_samples=model_quantity)\n\n        if outer_norm is None:\n            self._outer_norm = jnp.linalg.vector_norm\n        else:\n            self._outer_norm = outer_norm\n\n        self.data = data\n        self.tolerance = tolerance\n\n    def __call__(self, samples: dict[str, npt.ArrayLike]) -&gt; npt.ArrayLike:\n        \"\"\"\n        Evaluate the constraint, given RV samples.\n\n        Args:\n            samples: Mapping of RV (node) labels to drawn samples.\n\n        Returns:\n            Value of the constraint.\n\n        \"\"\"\n        return (\n            self._outer_norm(self._do_with_samples(**samples) - self.data)\n            - self.tolerance\n        )\n</code></pre>"},{"location":"api/#causalprog.causal_problem.Constraint.__call__","title":"<code>__call__(samples)</code>","text":"<p>Evaluate the constraint, given RV samples.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>dict[str, ArrayLike]</code> <p>Mapping of RV (node) labels to drawn samples.</p> required <p>Returns:</p> Type Description <code>ArrayLike</code> <p>Value of the constraint.</p> Source code in <code>src/causalprog/causal_problem/components.py</code> <pre><code>def __call__(self, samples: dict[str, npt.ArrayLike]) -&gt; npt.ArrayLike:\n    \"\"\"\n    Evaluate the constraint, given RV samples.\n\n    Args:\n        samples: Mapping of RV (node) labels to drawn samples.\n\n    Returns:\n        Value of the constraint.\n\n    \"\"\"\n    return (\n        self._outer_norm(self._do_with_samples(**samples) - self.data)\n        - self.tolerance\n    )\n</code></pre>"},{"location":"api/#causalprog.causal_problem.Constraint.__init__","title":"<code>__init__(*effect_handlers, model_quantity, outer_norm=None, data=0.0, tolerance=1e-06)</code>","text":"<p>Create a new constraint.</p> <p>Constraints have the form</p> \\[ c(\\theta) := \\mathrm{norm}\\left( g(\\theta) - g_{\\mathrm{data}} \\right) - \\epsilon \\] <p>where; - \\(\\mathrm{norm}\\) is the outer norm of the constraint (<code>outer_norm</code>), - \\(g(\\theta)\\) is the model quantity involved in the constraint     (<code>model_quantity</code>), - \\(g_{\\mathrm{data}}\\) is the observed data (<code>data</code>), - \\(\\epsilon\\) is the tolerance in the data (<code>tolerance</code>).</p> <p>In a causal problem, each constraint appears as the condition \\(c(\\theta)\\leq 0\\) in the minimisation / maximisation (hence the inclusion of the \\(-\\epsilon\\) term within \\(c(\\theta)\\) itself).</p> <p>\\(g\\) should be a (possibly vector-valued) function that acts on (a subset of) samples from the random variables of the causal problem. It must accept variable keyword-arguments only, and should access the samples for each random variable by indexing via the RV names (node labels). It should return the model quantity as computed from the samples, that \\(g_{\\mathrm{data}}\\) observed.</p> <p>\\(g_{\\mathrm{data}}\\) should be a fixed value whose shape is broadcast-able with the return shape of \\(g\\). It defaults to \\(0\\) if not explicitly set.</p> <p>\\(\\mathrm{norm}\\) should be a suitable norm to take on the difference between the model quantity as predicted by the samples (\\(g\\)) and the observed data (\\(g_{\\mathrm{data}}\\)). It must return a scalar value. The default is the 2-norm.</p> Source code in <code>src/causalprog/causal_problem/components.py</code> <pre><code>def __init__(\n    self,\n    *effect_handlers: ModelMask,\n    model_quantity: Callable[..., npt.ArrayLike],\n    outer_norm: Callable[[npt.ArrayLike], float] | None = None,\n    data: npt.ArrayLike = 0.0,\n    tolerance: float = 1.0e-6,\n) -&gt; None:\n    r\"\"\"\n    Create a new constraint.\n\n    Constraints have the form\n\n    $$ c(\\theta) :=\n    \\mathrm{norm}\\left( g(\\theta)\n    - g_{\\mathrm{data}} \\right)\n    - \\epsilon $$\n\n    where;\n    - $\\mathrm{norm}$ is the outer norm of the constraint (`outer_norm`),\n    - $g(\\theta)$ is the model quantity involved in the constraint\n        (`model_quantity`),\n    - $g_{\\mathrm{data}}$ is the observed data (`data`),\n    - $\\epsilon$ is the tolerance in the data (`tolerance`).\n\n    In a causal problem, each constraint appears as the condition $c(\\theta)\\leq 0$\n    in the minimisation / maximisation (hence the inclusion of the $-\\epsilon$\n    term within $c(\\theta)$ itself).\n\n    $g$ should be a (possibly vector-valued) function that acts on (a subset of)\n    samples from the random variables of the causal problem. It must accept\n    variable keyword-arguments only, and should access the samples for each random\n    variable by indexing via the RV names (node labels). It should return the\n    model quantity as computed from the samples, that $g_{\\mathrm{data}}$ observed.\n\n    $g_{\\mathrm{data}}$ should be a fixed value whose shape is broadcast-able with\n    the return shape of $g$. It defaults to $0$ if not explicitly set.\n\n    $\\mathrm{norm}$ should be a suitable norm to take on the difference between the\n    model quantity as predicted by the samples ($g$) and the observed data\n    ($g_{\\mathrm{data}}$). It must return a scalar value. The default is the 2-norm.\n    \"\"\"\n    super().__init__(*effect_handlers, do_with_samples=model_quantity)\n\n    if outer_norm is None:\n        self._outer_norm = jnp.linalg.vector_norm\n    else:\n        self._outer_norm = outer_norm\n\n    self.data = data\n    self.tolerance = tolerance\n</code></pre>"},{"location":"api/#causalprog.causal_problem.HandlerToApply","title":"<code>HandlerToApply</code>  <code>dataclass</code>","text":"<p>Specifies a handler that needs to be applied to a model at runtime.</p> Source code in <code>src/causalprog/causal_problem/handlers.py</code> <pre><code>@dataclass\nclass HandlerToApply:\n    \"\"\"Specifies a handler that needs to be applied to a model at runtime.\"\"\"\n\n    handler: EffectHandler\n    options: dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_pair(cls, pair: Sequence) -&gt; \"HandlerToApply\":\n        \"\"\"\n        Create an instance from an effect handler and its options.\n\n        The two objects should be passed in as the elements of a container of length\n        2. They can be passed in any order;\n        - One element must be a dictionary, which will be interpreted as the `options`\n            for the effect handler.\n        - The other element must be callable, and will be interpreted as the `handler`\n            itself.\n\n        Args:\n            pair: Container of two elements, one being the effect handler callable and\n                the other being the options to pass to it (as a dictionary).\n\n        Returns:\n            Class instance corresponding to the effect handler and options passed.\n\n        \"\"\"\n        if len(pair) != 2:  # noqa: PLR2004\n            msg = (\n                f\"{cls.__name__} can only be constructed from a container of 2 elements\"\n            )\n            raise ValueError(msg)\n\n        # __post_init__ will catch cases when the incorrect types for one or both items\n        # is passed, so we can just naively if-else here.\n        handler: EffectHandler\n        options: dict\n        if callable(pair[0]):\n            handler = pair[0]\n            options = pair[1]\n        else:\n            handler = pair[1]\n            options = pair[0]\n\n        return cls(handler=handler, options=options)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"\n        Validate set attributes.\n\n        - The handler is a callable object.\n        - The options have been passed as a dictionary of keyword-value pairs.\n        \"\"\"\n        if not callable(self.handler):\n            msg = f\"{type(self.handler).__name__} is not callable.\"\n            raise TypeError(msg)\n        if not isinstance(self.options, dict):\n            msg = (\n                \"Options should be dictionary mapping option arguments to values \"\n                f\"(got {type(self.options).__name__}).\"\n            )\n            raise TypeError(msg)\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Equality operation.\n\n        `HandlerToApply`s are considered equal if they use the same handler function and\n        provide the same options to this function.\n\n        Comparison to other types returns `False`.\n        \"\"\"\n        return (\n            isinstance(other, HandlerToApply)\n            and self.handler is other.handler\n            and self.options == other.options\n        )\n</code></pre>"},{"location":"api/#causalprog.causal_problem.HandlerToApply.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Equality operation.</p> <p><code>HandlerToApply</code>s are considered equal if they use the same handler function and provide the same options to this function.</p> <p>Comparison to other types returns <code>False</code>.</p> Source code in <code>src/causalprog/causal_problem/handlers.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Equality operation.\n\n    `HandlerToApply`s are considered equal if they use the same handler function and\n    provide the same options to this function.\n\n    Comparison to other types returns `False`.\n    \"\"\"\n    return (\n        isinstance(other, HandlerToApply)\n        and self.handler is other.handler\n        and self.options == other.options\n    )\n</code></pre>"},{"location":"api/#causalprog.causal_problem.HandlerToApply.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate set attributes.</p> <ul> <li>The handler is a callable object.</li> <li>The options have been passed as a dictionary of keyword-value pairs.</li> </ul> Source code in <code>src/causalprog/causal_problem/handlers.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"\n    Validate set attributes.\n\n    - The handler is a callable object.\n    - The options have been passed as a dictionary of keyword-value pairs.\n    \"\"\"\n    if not callable(self.handler):\n        msg = f\"{type(self.handler).__name__} is not callable.\"\n        raise TypeError(msg)\n    if not isinstance(self.options, dict):\n        msg = (\n            \"Options should be dictionary mapping option arguments to values \"\n            f\"(got {type(self.options).__name__}).\"\n        )\n        raise TypeError(msg)\n</code></pre>"},{"location":"api/#causalprog.causal_problem.HandlerToApply.from_pair","title":"<code>from_pair(pair)</code>  <code>classmethod</code>","text":"<p>Create an instance from an effect handler and its options.</p> <p>The two objects should be passed in as the elements of a container of length 2. They can be passed in any order; - One element must be a dictionary, which will be interpreted as the <code>options</code>     for the effect handler. - The other element must be callable, and will be interpreted as the <code>handler</code>     itself.</p> <p>Parameters:</p> Name Type Description Default <code>pair</code> <code>Sequence</code> <p>Container of two elements, one being the effect handler callable and the other being the options to pass to it (as a dictionary).</p> required <p>Returns:</p> Type Description <code>HandlerToApply</code> <p>Class instance corresponding to the effect handler and options passed.</p> Source code in <code>src/causalprog/causal_problem/handlers.py</code> <pre><code>@classmethod\ndef from_pair(cls, pair: Sequence) -&gt; \"HandlerToApply\":\n    \"\"\"\n    Create an instance from an effect handler and its options.\n\n    The two objects should be passed in as the elements of a container of length\n    2. They can be passed in any order;\n    - One element must be a dictionary, which will be interpreted as the `options`\n        for the effect handler.\n    - The other element must be callable, and will be interpreted as the `handler`\n        itself.\n\n    Args:\n        pair: Container of two elements, one being the effect handler callable and\n            the other being the options to pass to it (as a dictionary).\n\n    Returns:\n        Class instance corresponding to the effect handler and options passed.\n\n    \"\"\"\n    if len(pair) != 2:  # noqa: PLR2004\n        msg = (\n            f\"{cls.__name__} can only be constructed from a container of 2 elements\"\n        )\n        raise ValueError(msg)\n\n    # __post_init__ will catch cases when the incorrect types for one or both items\n    # is passed, so we can just naively if-else here.\n    handler: EffectHandler\n    options: dict\n    if callable(pair[0]):\n        handler = pair[0]\n        options = pair[1]\n    else:\n        handler = pair[1]\n        options = pair[0]\n\n    return cls(handler=handler, options=options)\n</code></pre>"},{"location":"api/#causalprog.causal_problem.causal_problem","title":"<code>causal_problem</code>","text":"<p>Classes for representing causal problems.</p>"},{"location":"api/#causalprog.causal_problem.causal_problem.CausalProblem","title":"<code>CausalProblem</code>","text":"<p>Defines a causal problem.</p> Source code in <code>src/causalprog/causal_problem/causal_problem.py</code> <pre><code>class CausalProblem:\n    \"\"\"Defines a causal problem.\"\"\"\n\n    _underlying_graph: Graph\n    causal_estimand: CausalEstimand\n    constraints: list[Constraint]\n\n    @property\n    def _ordered_components(self) -&gt; list[_CPComponent]:\n        \"\"\"Internal ordering for components of the causal problem.\"\"\"\n        return [*self.constraints, self.causal_estimand]\n\n    def __init__(\n        self,\n        graph: Graph,\n        *constraints: Constraint,\n        causal_estimand: CausalEstimand,\n    ) -&gt; None:\n        \"\"\"Create a new causal problem.\"\"\"\n        self._underlying_graph = graph\n        self.causal_estimand = causal_estimand\n        self.constraints = list(constraints)\n\n    def _associate_models_to_components(\n        self, n_samples: int\n    ) -&gt; tuple[list[Predictive], list[int]]:\n        \"\"\"\n        Create models to be used by components of the problem.\n\n        Depending on how many constraints (and the causal estimand) require effect\n        handlers to wrap `self._underlying_graph.model`, we will need to create several\n        predictive models to sample from. However, we also want to minimise the number\n        of such models we have to make, in order to minimise the time we spend\n        actually computing samples.\n\n        As such, in this method we determine:\n        - How many models we will need to build, by grouping the constraints and the\n          causal estimand by the handlers they use.\n        - Build these models, returning them in a list called `models`.\n        - Build another list that maps the index of components in\n          `self._ordered_components` to the index of the model in `models` that they\n          use. The causal estimand is by convention the component at index -1 of this\n          returned list.\n\n        Args:\n            n_samples: Value to be passed to `numpyro.Predictive`'s `num_samples`\n                argument for each of the models that are constructed from the underlying\n                graph.\n\n        Returns:\n            list[Predictive]: List of Predictive models, whose elements contain all the\n                models needed by the components.\n            list[int]: Mapping of component indexes (as per `self_ordered_components`)\n                to the index of the model in the first return argument that the\n                component uses.\n\n        \"\"\"\n        models: list[Predictive] = []\n        grouped_component_indexes: list[list[int]] = []\n        for index, component in enumerate(self._ordered_components):\n            # Determine if this constraint uses the same handlers as those of any of\n            # the other sets.\n            belongs_to_existing_group = False\n            for group in grouped_component_indexes:\n                # Pull any element from the group to compare models to.\n                # Items in a group are known to have the same model, so we can just\n                # pull out the first one.\n                group_element = self._ordered_components[group[0]]\n                # Check if the current constraint can also use this model.\n                if component.can_use_same_model_as(group_element):\n                    group.append(index)\n                    belongs_to_existing_group = True\n                    break\n\n            # If the component does not fit into any existing group, create a new\n            # group for it. And add the model corresponding to the group to the\n            # list of models.\n            if not belongs_to_existing_group:\n                grouped_component_indexes.append([index])\n\n                models.append(\n                    Predictive(\n                        component.apply_effects(self._underlying_graph.model),\n                        num_samples=n_samples,\n                    )\n                )\n\n        # Now \"invert\" the grouping, creating a mapping that maps the index of a\n        # component to the (index of the) model it uses.\n        component_index_to_model_index = []\n        for index in range(len(self._ordered_components)):\n            for group_index, group in enumerate(grouped_component_indexes):\n                if index in group:\n                    component_index_to_model_index.append(group_index)\n                    break\n        # All indexes should belong to at least one group (worst case scenario,\n        # their own individual group). Thus, it is safe to do the above to create\n        # the mapping from component index -&gt; model (group) index.\n        return models, component_index_to_model_index\n\n    def lagrangian(\n        self, n_samples: int = 1000, *, maximum_problem: bool = False\n    ) -&gt; Callable[[dict[str, npt.ArrayLike], npt.ArrayLike, jax.Array], npt.ArrayLike]:\n        \"\"\"\n        Return a function that evaluates the Lagrangian of this `CausalProblem`.\n\n        Following the\n        [KKT theorem](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions),\n        given the causal estimand and the constraints we can assemble a Lagrangian and\n        seek its stationary points, to in turn identify minimisers of the constrained\n        optimisation problem that we started with.\n\n        The Lagrangian returned is a mathematical function of its first two arguments.\n        The first argument is the same dictionary of parameters that is passed to models\n        like `Graph.model`, and is the values the parameters (represented by the\n        `ParameterNode`s) are taking. The second argument is a 1D vector of Lagrange\n        multipliers, whose length is equal to the number of constraints.\n\n        The remaining argument of the Lagrangian is the PRNG Key that should be used\n        when drawing samples.\n\n        Note that our current implementation assumes there are no equality constraints\n        being imposed (in which case, we would need a 3-argument Lagrangian function).\n\n        Args:\n            n_samples: The number of random samples to be drawn when estimating the\n                value of functions of the RVs.\n            maximum_problem: If passed as `True`, assemble the Lagrangian for the\n                maximisation problem. Otherwise assemble that for the minimisation\n                problem (default behaviour).\n\n        Returns:\n            The Lagrangian, as a function of the model parameters, Lagrange multipliers,\n                and PRNG key.\n\n        \"\"\"\n        maximisation_prefactor = -1.0 if maximum_problem else 1.0\n\n        # Build association between self.constraints and the model-samples that each\n        # one needs to use. We do this here, since once it is constructed, it is\n        # fixed, and doesn't need to be done each time we call the Lagrangian.\n        models, component_to_index_mapping = self._associate_models_to_components(\n            n_samples\n        )\n\n        def _inner(\n            parameter_values: dict[str, npt.ArrayLike],\n            l_mult: jax.Array,\n            rng_key: jax.Array,\n        ) -&gt; npt.ArrayLike:\n            # Draw samples from all models\n            all_samples = tuple(\n                sample_model(model, rng_key, parameter_values) for model in models\n            )\n\n            value = maximisation_prefactor * self.causal_estimand(all_samples[-1])\n            # TODO: https://github.com/UCL/causalprog/issues/87\n            value += sum(\n                l_mult[i] * c(all_samples[component_to_index_mapping[i]])\n                for i, c in enumerate(self.constraints)\n            )\n            return value\n\n        return _inner\n</code></pre>"},{"location":"api/#causalprog.causal_problem.causal_problem.CausalProblem.__init__","title":"<code>__init__(graph, *constraints, causal_estimand)</code>","text":"<p>Create a new causal problem.</p> Source code in <code>src/causalprog/causal_problem/causal_problem.py</code> <pre><code>def __init__(\n    self,\n    graph: Graph,\n    *constraints: Constraint,\n    causal_estimand: CausalEstimand,\n) -&gt; None:\n    \"\"\"Create a new causal problem.\"\"\"\n    self._underlying_graph = graph\n    self.causal_estimand = causal_estimand\n    self.constraints = list(constraints)\n</code></pre>"},{"location":"api/#causalprog.causal_problem.causal_problem.CausalProblem.lagrangian","title":"<code>lagrangian(n_samples=1000, *, maximum_problem=False)</code>","text":"<p>Return a function that evaluates the Lagrangian of this <code>CausalProblem</code>.</p> <p>Following the KKT theorem, given the causal estimand and the constraints we can assemble a Lagrangian and seek its stationary points, to in turn identify minimisers of the constrained optimisation problem that we started with.</p> <p>The Lagrangian returned is a mathematical function of its first two arguments. The first argument is the same dictionary of parameters that is passed to models like <code>Graph.model</code>, and is the values the parameters (represented by the <code>ParameterNode</code>s) are taking. The second argument is a 1D vector of Lagrange multipliers, whose length is equal to the number of constraints.</p> <p>The remaining argument of the Lagrangian is the PRNG Key that should be used when drawing samples.</p> <p>Note that our current implementation assumes there are no equality constraints being imposed (in which case, we would need a 3-argument Lagrangian function).</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of random samples to be drawn when estimating the value of functions of the RVs.</p> <code>1000</code> <code>maximum_problem</code> <code>bool</code> <p>If passed as <code>True</code>, assemble the Lagrangian for the maximisation problem. Otherwise assemble that for the minimisation problem (default behaviour).</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable[[dict[str, ArrayLike], ArrayLike, Array], ArrayLike]</code> <p>The Lagrangian, as a function of the model parameters, Lagrange multipliers, and PRNG key.</p> Source code in <code>src/causalprog/causal_problem/causal_problem.py</code> <pre><code>def lagrangian(\n    self, n_samples: int = 1000, *, maximum_problem: bool = False\n) -&gt; Callable[[dict[str, npt.ArrayLike], npt.ArrayLike, jax.Array], npt.ArrayLike]:\n    \"\"\"\n    Return a function that evaluates the Lagrangian of this `CausalProblem`.\n\n    Following the\n    [KKT theorem](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions),\n    given the causal estimand and the constraints we can assemble a Lagrangian and\n    seek its stationary points, to in turn identify minimisers of the constrained\n    optimisation problem that we started with.\n\n    The Lagrangian returned is a mathematical function of its first two arguments.\n    The first argument is the same dictionary of parameters that is passed to models\n    like `Graph.model`, and is the values the parameters (represented by the\n    `ParameterNode`s) are taking. The second argument is a 1D vector of Lagrange\n    multipliers, whose length is equal to the number of constraints.\n\n    The remaining argument of the Lagrangian is the PRNG Key that should be used\n    when drawing samples.\n\n    Note that our current implementation assumes there are no equality constraints\n    being imposed (in which case, we would need a 3-argument Lagrangian function).\n\n    Args:\n        n_samples: The number of random samples to be drawn when estimating the\n            value of functions of the RVs.\n        maximum_problem: If passed as `True`, assemble the Lagrangian for the\n            maximisation problem. Otherwise assemble that for the minimisation\n            problem (default behaviour).\n\n    Returns:\n        The Lagrangian, as a function of the model parameters, Lagrange multipliers,\n            and PRNG key.\n\n    \"\"\"\n    maximisation_prefactor = -1.0 if maximum_problem else 1.0\n\n    # Build association between self.constraints and the model-samples that each\n    # one needs to use. We do this here, since once it is constructed, it is\n    # fixed, and doesn't need to be done each time we call the Lagrangian.\n    models, component_to_index_mapping = self._associate_models_to_components(\n        n_samples\n    )\n\n    def _inner(\n        parameter_values: dict[str, npt.ArrayLike],\n        l_mult: jax.Array,\n        rng_key: jax.Array,\n    ) -&gt; npt.ArrayLike:\n        # Draw samples from all models\n        all_samples = tuple(\n            sample_model(model, rng_key, parameter_values) for model in models\n        )\n\n        value = maximisation_prefactor * self.causal_estimand(all_samples[-1])\n        # TODO: https://github.com/UCL/causalprog/issues/87\n        value += sum(\n            l_mult[i] * c(all_samples[component_to_index_mapping[i]])\n            for i, c in enumerate(self.constraints)\n        )\n        return value\n\n    return _inner\n</code></pre>"},{"location":"api/#causalprog.causal_problem.causal_problem.sample_model","title":"<code>sample_model(model, rng_key, parameter_values)</code>","text":"<p>Draw samples from the predictive model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Predictive</code> <p>Predictive model to draw samples from.</p> required <code>rng_key</code> <code>Array</code> <p>PRNG Key to use in pseudorandom number generation.</p> required <code>parameter_values</code> <code>dict[str, ArrayLike]</code> <p>Model parameter values to substitute.</p> required <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p><code>dict</code> of samples, with RV labels as keys and sample values (<code>jax.Array</code>s) as values.</p> Source code in <code>src/causalprog/causal_problem/causal_problem.py</code> <pre><code>def sample_model(\n    model: Predictive, rng_key: jax.Array, parameter_values: dict[str, npt.ArrayLike]\n) -&gt; dict[str, npt.ArrayLike]:\n    \"\"\"\n    Draw samples from the predictive model.\n\n    Args:\n        model: Predictive model to draw samples from.\n        rng_key: PRNG Key to use in pseudorandom number generation.\n        parameter_values: Model parameter values to substitute.\n\n    Returns:\n        `dict` of samples, with RV labels as keys and sample values (`jax.Array`s) as\n            values.\n\n    \"\"\"\n    return jax.vmap(lambda pv, key: model(key, **pv), in_axes=(None, 0))(\n        parameter_values,\n        jax.random.split(rng_key, model.num_samples),\n    )\n</code></pre>"},{"location":"api/#causalprog.causal_problem.components","title":"<code>components</code>","text":"<p>Classes for defining causal estimands and constraints of causal problems.</p>"},{"location":"api/#causalprog.causal_problem.components.CausalEstimand","title":"<code>CausalEstimand</code>","text":"<p>               Bases: <code>_CPComponent</code></p> <p>A Causal Estimand.</p> <p>The causal estimand is the function that we want to minimise (and maximise) as part of a causal problem. It should be a scalar-valued function of the random variables appearing in a graph.</p> Source code in <code>src/causalprog/causal_problem/components.py</code> <pre><code>class CausalEstimand(_CPComponent):\n    \"\"\"\n    A Causal Estimand.\n\n    The causal estimand is the function that we want to minimise (and maximise)\n    as part of a causal problem. It should be a scalar-valued function of the\n    random variables appearing in a graph.\n    \"\"\"\n</code></pre>"},{"location":"api/#causalprog.causal_problem.components.Constraint","title":"<code>Constraint</code>","text":"<p>               Bases: <code>_CPComponent</code></p> <p>A Constraint that forms part of a causal problem.</p> <p>Constraints of a causal problem are derived properties of RVs for which we have observed data. The causal estimand is minimised (or maximised) subject to the predicted values of the constraints being close to their observed values in the data.</p> <p>Adding a constraint \\(g(\\theta)\\) to a causal problem (where \\(\\theta\\) are the parameters of the causal problem) essentially imposes an additional constraint on the minimisation problem;</p> \\[ g(\\theta) - g_{\\text{data}} \\leq \\epsilon, \\] <p>where \\(g_{\\text{data}}\\) is the observed data value for the quantity \\(g\\), and \\(\\epsilon\\) is some tolerance.</p> Source code in <code>src/causalprog/causal_problem/components.py</code> <pre><code>class Constraint(_CPComponent):\n    r\"\"\"\n    A Constraint that forms part of a causal problem.\n\n    Constraints of a causal problem are derived properties of RVs for which we\n    have observed data. The causal estimand is minimised (or maximised) subject\n    to the predicted values of the constraints being close to their observed\n    values in the data.\n\n    Adding a constraint $g(\\theta)$ to a causal problem (where $\\theta$ are the\n    parameters of the causal problem) essentially imposes an additional\n    constraint on the minimisation problem;\n\n    $$ g(\\theta) - g_{\\text{data}} \\leq \\epsilon, $$\n\n    where $g_{\\text{data}}$ is the observed data value for the quantity $g$,\n    and $\\epsilon$ is some tolerance.\n    \"\"\"\n\n    data: npt.ArrayLike\n    tolerance: npt.ArrayLike\n    _outer_norm: Callable[[npt.ArrayLike], float]\n\n    def __init__(\n        self,\n        *effect_handlers: ModelMask,\n        model_quantity: Callable[..., npt.ArrayLike],\n        outer_norm: Callable[[npt.ArrayLike], float] | None = None,\n        data: npt.ArrayLike = 0.0,\n        tolerance: float = 1.0e-6,\n    ) -&gt; None:\n        r\"\"\"\n        Create a new constraint.\n\n        Constraints have the form\n\n        $$ c(\\theta) :=\n        \\mathrm{norm}\\left( g(\\theta)\n        - g_{\\mathrm{data}} \\right)\n        - \\epsilon $$\n\n        where;\n        - $\\mathrm{norm}$ is the outer norm of the constraint (`outer_norm`),\n        - $g(\\theta)$ is the model quantity involved in the constraint\n            (`model_quantity`),\n        - $g_{\\mathrm{data}}$ is the observed data (`data`),\n        - $\\epsilon$ is the tolerance in the data (`tolerance`).\n\n        In a causal problem, each constraint appears as the condition $c(\\theta)\\leq 0$\n        in the minimisation / maximisation (hence the inclusion of the $-\\epsilon$\n        term within $c(\\theta)$ itself).\n\n        $g$ should be a (possibly vector-valued) function that acts on (a subset of)\n        samples from the random variables of the causal problem. It must accept\n        variable keyword-arguments only, and should access the samples for each random\n        variable by indexing via the RV names (node labels). It should return the\n        model quantity as computed from the samples, that $g_{\\mathrm{data}}$ observed.\n\n        $g_{\\mathrm{data}}$ should be a fixed value whose shape is broadcast-able with\n        the return shape of $g$. It defaults to $0$ if not explicitly set.\n\n        $\\mathrm{norm}$ should be a suitable norm to take on the difference between the\n        model quantity as predicted by the samples ($g$) and the observed data\n        ($g_{\\mathrm{data}}$). It must return a scalar value. The default is the 2-norm.\n        \"\"\"\n        super().__init__(*effect_handlers, do_with_samples=model_quantity)\n\n        if outer_norm is None:\n            self._outer_norm = jnp.linalg.vector_norm\n        else:\n            self._outer_norm = outer_norm\n\n        self.data = data\n        self.tolerance = tolerance\n\n    def __call__(self, samples: dict[str, npt.ArrayLike]) -&gt; npt.ArrayLike:\n        \"\"\"\n        Evaluate the constraint, given RV samples.\n\n        Args:\n            samples: Mapping of RV (node) labels to drawn samples.\n\n        Returns:\n            Value of the constraint.\n\n        \"\"\"\n        return (\n            self._outer_norm(self._do_with_samples(**samples) - self.data)\n            - self.tolerance\n        )\n</code></pre>"},{"location":"api/#causalprog.causal_problem.components.Constraint.__call__","title":"<code>__call__(samples)</code>","text":"<p>Evaluate the constraint, given RV samples.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>dict[str, ArrayLike]</code> <p>Mapping of RV (node) labels to drawn samples.</p> required <p>Returns:</p> Type Description <code>ArrayLike</code> <p>Value of the constraint.</p> Source code in <code>src/causalprog/causal_problem/components.py</code> <pre><code>def __call__(self, samples: dict[str, npt.ArrayLike]) -&gt; npt.ArrayLike:\n    \"\"\"\n    Evaluate the constraint, given RV samples.\n\n    Args:\n        samples: Mapping of RV (node) labels to drawn samples.\n\n    Returns:\n        Value of the constraint.\n\n    \"\"\"\n    return (\n        self._outer_norm(self._do_with_samples(**samples) - self.data)\n        - self.tolerance\n    )\n</code></pre>"},{"location":"api/#causalprog.causal_problem.components.Constraint.__init__","title":"<code>__init__(*effect_handlers, model_quantity, outer_norm=None, data=0.0, tolerance=1e-06)</code>","text":"<p>Create a new constraint.</p> <p>Constraints have the form</p> \\[ c(\\theta) := \\mathrm{norm}\\left( g(\\theta) - g_{\\mathrm{data}} \\right) - \\epsilon \\] <p>where; - \\(\\mathrm{norm}\\) is the outer norm of the constraint (<code>outer_norm</code>), - \\(g(\\theta)\\) is the model quantity involved in the constraint     (<code>model_quantity</code>), - \\(g_{\\mathrm{data}}\\) is the observed data (<code>data</code>), - \\(\\epsilon\\) is the tolerance in the data (<code>tolerance</code>).</p> <p>In a causal problem, each constraint appears as the condition \\(c(\\theta)\\leq 0\\) in the minimisation / maximisation (hence the inclusion of the \\(-\\epsilon\\) term within \\(c(\\theta)\\) itself).</p> <p>\\(g\\) should be a (possibly vector-valued) function that acts on (a subset of) samples from the random variables of the causal problem. It must accept variable keyword-arguments only, and should access the samples for each random variable by indexing via the RV names (node labels). It should return the model quantity as computed from the samples, that \\(g_{\\mathrm{data}}\\) observed.</p> <p>\\(g_{\\mathrm{data}}\\) should be a fixed value whose shape is broadcast-able with the return shape of \\(g\\). It defaults to \\(0\\) if not explicitly set.</p> <p>\\(\\mathrm{norm}\\) should be a suitable norm to take on the difference between the model quantity as predicted by the samples (\\(g\\)) and the observed data (\\(g_{\\mathrm{data}}\\)). It must return a scalar value. The default is the 2-norm.</p> Source code in <code>src/causalprog/causal_problem/components.py</code> <pre><code>def __init__(\n    self,\n    *effect_handlers: ModelMask,\n    model_quantity: Callable[..., npt.ArrayLike],\n    outer_norm: Callable[[npt.ArrayLike], float] | None = None,\n    data: npt.ArrayLike = 0.0,\n    tolerance: float = 1.0e-6,\n) -&gt; None:\n    r\"\"\"\n    Create a new constraint.\n\n    Constraints have the form\n\n    $$ c(\\theta) :=\n    \\mathrm{norm}\\left( g(\\theta)\n    - g_{\\mathrm{data}} \\right)\n    - \\epsilon $$\n\n    where;\n    - $\\mathrm{norm}$ is the outer norm of the constraint (`outer_norm`),\n    - $g(\\theta)$ is the model quantity involved in the constraint\n        (`model_quantity`),\n    - $g_{\\mathrm{data}}$ is the observed data (`data`),\n    - $\\epsilon$ is the tolerance in the data (`tolerance`).\n\n    In a causal problem, each constraint appears as the condition $c(\\theta)\\leq 0$\n    in the minimisation / maximisation (hence the inclusion of the $-\\epsilon$\n    term within $c(\\theta)$ itself).\n\n    $g$ should be a (possibly vector-valued) function that acts on (a subset of)\n    samples from the random variables of the causal problem. It must accept\n    variable keyword-arguments only, and should access the samples for each random\n    variable by indexing via the RV names (node labels). It should return the\n    model quantity as computed from the samples, that $g_{\\mathrm{data}}$ observed.\n\n    $g_{\\mathrm{data}}$ should be a fixed value whose shape is broadcast-able with\n    the return shape of $g$. It defaults to $0$ if not explicitly set.\n\n    $\\mathrm{norm}$ should be a suitable norm to take on the difference between the\n    model quantity as predicted by the samples ($g$) and the observed data\n    ($g_{\\mathrm{data}}$). It must return a scalar value. The default is the 2-norm.\n    \"\"\"\n    super().__init__(*effect_handlers, do_with_samples=model_quantity)\n\n    if outer_norm is None:\n        self._outer_norm = jnp.linalg.vector_norm\n    else:\n        self._outer_norm = outer_norm\n\n    self.data = data\n    self.tolerance = tolerance\n</code></pre>"},{"location":"api/#causalprog.causal_problem.handlers","title":"<code>handlers</code>","text":"<p>Container class for specifying effect handlers that need to be applied at runtime.</p>"},{"location":"api/#causalprog.causal_problem.handlers.HandlerToApply","title":"<code>HandlerToApply</code>  <code>dataclass</code>","text":"<p>Specifies a handler that needs to be applied to a model at runtime.</p> Source code in <code>src/causalprog/causal_problem/handlers.py</code> <pre><code>@dataclass\nclass HandlerToApply:\n    \"\"\"Specifies a handler that needs to be applied to a model at runtime.\"\"\"\n\n    handler: EffectHandler\n    options: dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_pair(cls, pair: Sequence) -&gt; \"HandlerToApply\":\n        \"\"\"\n        Create an instance from an effect handler and its options.\n\n        The two objects should be passed in as the elements of a container of length\n        2. They can be passed in any order;\n        - One element must be a dictionary, which will be interpreted as the `options`\n            for the effect handler.\n        - The other element must be callable, and will be interpreted as the `handler`\n            itself.\n\n        Args:\n            pair: Container of two elements, one being the effect handler callable and\n                the other being the options to pass to it (as a dictionary).\n\n        Returns:\n            Class instance corresponding to the effect handler and options passed.\n\n        \"\"\"\n        if len(pair) != 2:  # noqa: PLR2004\n            msg = (\n                f\"{cls.__name__} can only be constructed from a container of 2 elements\"\n            )\n            raise ValueError(msg)\n\n        # __post_init__ will catch cases when the incorrect types for one or both items\n        # is passed, so we can just naively if-else here.\n        handler: EffectHandler\n        options: dict\n        if callable(pair[0]):\n            handler = pair[0]\n            options = pair[1]\n        else:\n            handler = pair[1]\n            options = pair[0]\n\n        return cls(handler=handler, options=options)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"\n        Validate set attributes.\n\n        - The handler is a callable object.\n        - The options have been passed as a dictionary of keyword-value pairs.\n        \"\"\"\n        if not callable(self.handler):\n            msg = f\"{type(self.handler).__name__} is not callable.\"\n            raise TypeError(msg)\n        if not isinstance(self.options, dict):\n            msg = (\n                \"Options should be dictionary mapping option arguments to values \"\n                f\"(got {type(self.options).__name__}).\"\n            )\n            raise TypeError(msg)\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Equality operation.\n\n        `HandlerToApply`s are considered equal if they use the same handler function and\n        provide the same options to this function.\n\n        Comparison to other types returns `False`.\n        \"\"\"\n        return (\n            isinstance(other, HandlerToApply)\n            and self.handler is other.handler\n            and self.options == other.options\n        )\n</code></pre>"},{"location":"api/#causalprog.causal_problem.handlers.HandlerToApply.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Equality operation.</p> <p><code>HandlerToApply</code>s are considered equal if they use the same handler function and provide the same options to this function.</p> <p>Comparison to other types returns <code>False</code>.</p> Source code in <code>src/causalprog/causal_problem/handlers.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Equality operation.\n\n    `HandlerToApply`s are considered equal if they use the same handler function and\n    provide the same options to this function.\n\n    Comparison to other types returns `False`.\n    \"\"\"\n    return (\n        isinstance(other, HandlerToApply)\n        and self.handler is other.handler\n        and self.options == other.options\n    )\n</code></pre>"},{"location":"api/#causalprog.causal_problem.handlers.HandlerToApply.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate set attributes.</p> <ul> <li>The handler is a callable object.</li> <li>The options have been passed as a dictionary of keyword-value pairs.</li> </ul> Source code in <code>src/causalprog/causal_problem/handlers.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"\n    Validate set attributes.\n\n    - The handler is a callable object.\n    - The options have been passed as a dictionary of keyword-value pairs.\n    \"\"\"\n    if not callable(self.handler):\n        msg = f\"{type(self.handler).__name__} is not callable.\"\n        raise TypeError(msg)\n    if not isinstance(self.options, dict):\n        msg = (\n            \"Options should be dictionary mapping option arguments to values \"\n            f\"(got {type(self.options).__name__}).\"\n        )\n        raise TypeError(msg)\n</code></pre>"},{"location":"api/#causalprog.causal_problem.handlers.HandlerToApply.from_pair","title":"<code>from_pair(pair)</code>  <code>classmethod</code>","text":"<p>Create an instance from an effect handler and its options.</p> <p>The two objects should be passed in as the elements of a container of length 2. They can be passed in any order; - One element must be a dictionary, which will be interpreted as the <code>options</code>     for the effect handler. - The other element must be callable, and will be interpreted as the <code>handler</code>     itself.</p> <p>Parameters:</p> Name Type Description Default <code>pair</code> <code>Sequence</code> <p>Container of two elements, one being the effect handler callable and the other being the options to pass to it (as a dictionary).</p> required <p>Returns:</p> Type Description <code>HandlerToApply</code> <p>Class instance corresponding to the effect handler and options passed.</p> Source code in <code>src/causalprog/causal_problem/handlers.py</code> <pre><code>@classmethod\ndef from_pair(cls, pair: Sequence) -&gt; \"HandlerToApply\":\n    \"\"\"\n    Create an instance from an effect handler and its options.\n\n    The two objects should be passed in as the elements of a container of length\n    2. They can be passed in any order;\n    - One element must be a dictionary, which will be interpreted as the `options`\n        for the effect handler.\n    - The other element must be callable, and will be interpreted as the `handler`\n        itself.\n\n    Args:\n        pair: Container of two elements, one being the effect handler callable and\n            the other being the options to pass to it (as a dictionary).\n\n    Returns:\n        Class instance corresponding to the effect handler and options passed.\n\n    \"\"\"\n    if len(pair) != 2:  # noqa: PLR2004\n        msg = (\n            f\"{cls.__name__} can only be constructed from a container of 2 elements\"\n        )\n        raise ValueError(msg)\n\n    # __post_init__ will catch cases when the incorrect types for one or both items\n    # is passed, so we can just naively if-else here.\n    handler: EffectHandler\n    options: dict\n    if callable(pair[0]):\n        handler = pair[0]\n        options = pair[1]\n    else:\n        handler = pair[1]\n        options = pair[0]\n\n    return cls(handler=handler, options=options)\n</code></pre>"},{"location":"api/#causalprog.graph","title":"<code>graph</code>","text":"<p>Creation and storage of graphs.</p>"},{"location":"api/#causalprog.graph.graph","title":"<code>graph</code>","text":"<p>Graph storage.</p>"},{"location":"api/#causalprog.graph.graph.Graph","title":"<code>Graph</code>","text":"<p>               Bases: <code>Labelled</code></p> <p>A directed acyclic graph that represents a causality tree.</p> Source code in <code>src/causalprog/graph/graph.py</code> <pre><code>class Graph(Labelled):\n    \"\"\"A directed acyclic graph that represents a causality tree.\"\"\"\n\n    _nodes_by_label: dict[str, Node]\n\n    def __init__(self, *, label: str, graph: nx.DiGraph | None = None) -&gt; None:\n        \"\"\"\n        Create a graph.\n\n        Args:\n            label: A label to identify the graph\n            graph: A networkx graph to base this graph on\n\n        \"\"\"\n        super().__init__(label=label)\n        self._nodes_by_label = {}\n        if graph is None:\n            graph = nx.DiGraph()\n\n        self._graph = graph\n        for node in graph.nodes:\n            self._nodes_by_label[node.label] = node\n\n    def get_node(self, label: str) -&gt; Node:\n        \"\"\"\n        Get a node from its label.\n\n        Args:\n            label: The label\n\n        Returns:\n            The node\n\n        \"\"\"\n        node = self._nodes_by_label.get(label, None)\n        if not node:\n            msg = f'Node not found with label \"{label}\"'\n            raise KeyError(msg)\n        return node\n\n    def add_node(self, node: Node) -&gt; None:\n        \"\"\"\n        Add a node to the graph.\n\n        Args:\n            node: The node to add\n\n        \"\"\"\n        if node.label in self._nodes_by_label:\n            msg = f\"Duplicate node label: {node.label}\"\n            raise ValueError(msg)\n        self._nodes_by_label[node.label] = node\n        self._graph.add_node(node)\n\n    def add_edge(self, start_node: Node | str, end_node: Node | str) -&gt; None:\n        \"\"\"\n        Add a directed edge to the graph.\n\n        Adding an edge between nodes not currently in the graph,\n        will cause said nodes to be added to the graph along with\n        the edge.\n\n        Args:\n            start_node: The node that the edge points from\n            end_node: The node that the edge points to\n\n        \"\"\"\n        if isinstance(start_node, str):\n            start_node = self.get_node(start_node)\n        if isinstance(end_node, str):\n            end_node = self.get_node(end_node)\n        if start_node.label not in self._nodes_by_label:\n            self.add_node(start_node)\n        if end_node.label not in self._nodes_by_label:\n            self.add_node(end_node)\n        for node_to_check in (start_node, end_node):\n            if node_to_check != self._nodes_by_label[node_to_check.label]:\n                msg = \"Invalid node: {node_to_check}\"\n                raise ValueError(msg)\n        self._graph.add_edge(start_node, end_node)\n\n    @property\n    def parameter_nodes(self) -&gt; tuple[Node, ...]:\n        \"\"\"\n        Returns all parameter nodes in the graph.\n\n        The returned tuple uses the `ordered_nodes` property to obtain the parameter\n        nodes so that a natural \"fixed order\" is given to the parameters. When parameter\n        values are given as inputs to the causal estimand and / or constraint functions,\n        they will ideally be given as a single vector of parameter values, in which case\n        a fixed ordering for the parameters is necessary to make an association to the\n        components of the given input vector.\n\n        Returns:\n            Parameter nodes\n\n        \"\"\"\n        return tuple(node for node in self.ordered_nodes if node.is_parameter)\n\n    @property\n    def predecessors(self) -&gt; dict[Node, tuple[Node, ...]]:\n        \"\"\"\n        Get predecessors of every node.\n\n        Returns:\n            Mapping of each Node to its predecessor Nodes\n\n        \"\"\"\n        return {node: tuple(self._graph.predecessors(node)) for node in self.nodes}\n\n    @property\n    def successors(self) -&gt; dict[Node, tuple[Node, ...]]:\n        \"\"\"\n        Get successors of every node.\n\n        Returns:\n            Mapping of each Node to its successor Nodes.\n\n        \"\"\"\n        return {node: tuple(self._graph.successors(node)) for node in self.nodes}\n\n    @property\n    def nodes(self) -&gt; tuple[Node, ...]:\n        \"\"\"\n        Get the nodes of the graph, with no enforced ordering.\n\n        Returns:\n            A list of all the nodes in the graph.\n\n        See Also:\n            ordered_nodes: Fetch an ordered list of the nodes in the graph.\n\n        \"\"\"\n        return tuple(self._graph.nodes())\n\n    @property\n    def edges(self) -&gt; tuple[tuple[Node, Node], ...]:\n        \"\"\"\n        Get the edges of the graph.\n\n        Returns:\n            A tuple of all the edges in the graph.\n\n        \"\"\"\n        return tuple(self._graph.edges())\n\n    @property\n    def ordered_nodes(self) -&gt; tuple[Node, ...]:\n        \"\"\"\n        Nodes ordered so that each node appears after its dependencies.\n\n        Returns:\n            A list of all the nodes, ordered such that each node\n                appears after all its dependencies.\n\n        \"\"\"\n        if not nx.is_directed_acyclic_graph(self._graph):\n            msg = \"Graph is not acyclic.\"\n            raise RuntimeError(msg)\n        return tuple(nx.topological_sort(self._graph))\n\n    @property\n    def ordered_dist_nodes(self) -&gt; tuple[Node, ...]:\n        \"\"\"\n        `DistributionNode`s in dependency order.\n\n        Each `DistributionNode` in the returned list appears after all its\n        dependencies. Order is derived from `self.ordered_nodes`, selecting\n        only those nodes where `is_distribution` is `True`.\n        \"\"\"\n        return tuple(node for node in self.ordered_nodes if node.is_distribution)\n\n    def roots_down_to_outcome(\n        self,\n        outcome_node_label: str,\n    ) -&gt; tuple[Node, ...]:\n        \"\"\"\n        Get ordered list of nodes that outcome depends on.\n\n        Nodes are ordered so that each node appears after its dependencies.\n\n        Args:\n            outcome_node_label: The label of the outcome node\n\n        Returns:\n            A list of the nodes, ordered from root nodes to the outcome Node.\n\n        \"\"\"\n        outcome = self.get_node(outcome_node_label)\n        ancestors = nx.ancestors(self._graph, outcome)\n        return tuple(\n            node for node in self.ordered_nodes if node == outcome or node in ancestors\n        )\n\n    def model(self, **parameter_values: npt.ArrayLike) -&gt; dict[str, npt.ArrayLike]:\n        \"\"\"\n        Model corresponding to the `Graph`'s structure.\n\n        The model created takes values of the nodes that are parameter as keyword\n        arguments. Names of the keyword arguments should match the labels of the\n        `ParameterNode`s, and their values should be the values of those parameters.\n\n        The method returns a dictionary recording the mode sites that are created.\n        This means that the model can be 'extended' further by defining additional\n        sites in a wrapper around this method.\n\n        Args:\n            parameter_values: Names of the keyword arguments should match the labels\n                of the `ParameterNode`s, and their values should be the values of those\n                parameters.\n\n        Returns:\n            Mapping of non-`ParameterNode` `Node` labels to the site objects created\n                for these nodes.\n\n        \"\"\"\n        # Confirm that all `ParameterNode`s have been assigned a value.\n        for node in self.parameter_nodes:\n            if node.label not in parameter_values:\n                msg = f\"ParameterNode '{node.label}' not assigned\"\n                raise KeyError(msg)\n\n        # Build model sequentially, using the node_order to inform the\n        # construction process.\n        node_record: dict[str, npt.ArrayLike] = {}\n        for node in self.ordered_dist_nodes:\n            node_record[node.label] = node.create_model_site(\n                **parameter_values,  # All nodes require knowledge of the parameters\n                **node_record,  # and any dependent nodes we have already visited\n            )\n\n        return node_record\n</code></pre>"},{"location":"api/#causalprog.graph.graph.Graph.edges","title":"<code>edges</code>  <code>property</code>","text":"<p>Get the edges of the graph.</p> <p>Returns:</p> Type Description <code>tuple[tuple[Node, Node], ...]</code> <p>A tuple of all the edges in the graph.</p>"},{"location":"api/#causalprog.graph.graph.Graph.nodes","title":"<code>nodes</code>  <code>property</code>","text":"<p>Get the nodes of the graph, with no enforced ordering.</p> <p>Returns:</p> Type Description <code>tuple[Node, ...]</code> <p>A list of all the nodes in the graph.</p> See Also <p>ordered_nodes: Fetch an ordered list of the nodes in the graph.</p>"},{"location":"api/#causalprog.graph.graph.Graph.ordered_dist_nodes","title":"<code>ordered_dist_nodes</code>  <code>property</code>","text":"<p><code>DistributionNode</code>s in dependency order.</p> <p>Each <code>DistributionNode</code> in the returned list appears after all its dependencies. Order is derived from <code>self.ordered_nodes</code>, selecting only those nodes where <code>is_distribution</code> is <code>True</code>.</p>"},{"location":"api/#causalprog.graph.graph.Graph.ordered_nodes","title":"<code>ordered_nodes</code>  <code>property</code>","text":"<p>Nodes ordered so that each node appears after its dependencies.</p> <p>Returns:</p> Type Description <code>tuple[Node, ...]</code> <p>A list of all the nodes, ordered such that each node appears after all its dependencies.</p>"},{"location":"api/#causalprog.graph.graph.Graph.parameter_nodes","title":"<code>parameter_nodes</code>  <code>property</code>","text":"<p>Returns all parameter nodes in the graph.</p> <p>The returned tuple uses the <code>ordered_nodes</code> property to obtain the parameter nodes so that a natural \"fixed order\" is given to the parameters. When parameter values are given as inputs to the causal estimand and / or constraint functions, they will ideally be given as a single vector of parameter values, in which case a fixed ordering for the parameters is necessary to make an association to the components of the given input vector.</p> <p>Returns:</p> Type Description <code>tuple[Node, ...]</code> <p>Parameter nodes</p>"},{"location":"api/#causalprog.graph.graph.Graph.predecessors","title":"<code>predecessors</code>  <code>property</code>","text":"<p>Get predecessors of every node.</p> <p>Returns:</p> Type Description <code>dict[Node, tuple[Node, ...]]</code> <p>Mapping of each Node to its predecessor Nodes</p>"},{"location":"api/#causalprog.graph.graph.Graph.successors","title":"<code>successors</code>  <code>property</code>","text":"<p>Get successors of every node.</p> <p>Returns:</p> Type Description <code>dict[Node, tuple[Node, ...]]</code> <p>Mapping of each Node to its successor Nodes.</p>"},{"location":"api/#causalprog.graph.graph.Graph.__init__","title":"<code>__init__(*, label, graph=None)</code>","text":"<p>Create a graph.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>A label to identify the graph</p> required <code>graph</code> <code>DiGraph | None</code> <p>A networkx graph to base this graph on</p> <code>None</code> Source code in <code>src/causalprog/graph/graph.py</code> <pre><code>def __init__(self, *, label: str, graph: nx.DiGraph | None = None) -&gt; None:\n    \"\"\"\n    Create a graph.\n\n    Args:\n        label: A label to identify the graph\n        graph: A networkx graph to base this graph on\n\n    \"\"\"\n    super().__init__(label=label)\n    self._nodes_by_label = {}\n    if graph is None:\n        graph = nx.DiGraph()\n\n    self._graph = graph\n    for node in graph.nodes:\n        self._nodes_by_label[node.label] = node\n</code></pre>"},{"location":"api/#causalprog.graph.graph.Graph.add_edge","title":"<code>add_edge(start_node, end_node)</code>","text":"<p>Add a directed edge to the graph.</p> <p>Adding an edge between nodes not currently in the graph, will cause said nodes to be added to the graph along with the edge.</p> <p>Parameters:</p> Name Type Description Default <code>start_node</code> <code>Node | str</code> <p>The node that the edge points from</p> required <code>end_node</code> <code>Node | str</code> <p>The node that the edge points to</p> required Source code in <code>src/causalprog/graph/graph.py</code> <pre><code>def add_edge(self, start_node: Node | str, end_node: Node | str) -&gt; None:\n    \"\"\"\n    Add a directed edge to the graph.\n\n    Adding an edge between nodes not currently in the graph,\n    will cause said nodes to be added to the graph along with\n    the edge.\n\n    Args:\n        start_node: The node that the edge points from\n        end_node: The node that the edge points to\n\n    \"\"\"\n    if isinstance(start_node, str):\n        start_node = self.get_node(start_node)\n    if isinstance(end_node, str):\n        end_node = self.get_node(end_node)\n    if start_node.label not in self._nodes_by_label:\n        self.add_node(start_node)\n    if end_node.label not in self._nodes_by_label:\n        self.add_node(end_node)\n    for node_to_check in (start_node, end_node):\n        if node_to_check != self._nodes_by_label[node_to_check.label]:\n            msg = \"Invalid node: {node_to_check}\"\n            raise ValueError(msg)\n    self._graph.add_edge(start_node, end_node)\n</code></pre>"},{"location":"api/#causalprog.graph.graph.Graph.add_node","title":"<code>add_node(node)</code>","text":"<p>Add a node to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node to add</p> required Source code in <code>src/causalprog/graph/graph.py</code> <pre><code>def add_node(self, node: Node) -&gt; None:\n    \"\"\"\n    Add a node to the graph.\n\n    Args:\n        node: The node to add\n\n    \"\"\"\n    if node.label in self._nodes_by_label:\n        msg = f\"Duplicate node label: {node.label}\"\n        raise ValueError(msg)\n    self._nodes_by_label[node.label] = node\n    self._graph.add_node(node)\n</code></pre>"},{"location":"api/#causalprog.graph.graph.Graph.get_node","title":"<code>get_node(label)</code>","text":"<p>Get a node from its label.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>The label</p> required <p>Returns:</p> Type Description <code>Node</code> <p>The node</p> Source code in <code>src/causalprog/graph/graph.py</code> <pre><code>def get_node(self, label: str) -&gt; Node:\n    \"\"\"\n    Get a node from its label.\n\n    Args:\n        label: The label\n\n    Returns:\n        The node\n\n    \"\"\"\n    node = self._nodes_by_label.get(label, None)\n    if not node:\n        msg = f'Node not found with label \"{label}\"'\n        raise KeyError(msg)\n    return node\n</code></pre>"},{"location":"api/#causalprog.graph.graph.Graph.model","title":"<code>model(**parameter_values)</code>","text":"<p>Model corresponding to the <code>Graph</code>'s structure.</p> <p>The model created takes values of the nodes that are parameter as keyword arguments. Names of the keyword arguments should match the labels of the <code>ParameterNode</code>s, and their values should be the values of those parameters.</p> <p>The method returns a dictionary recording the mode sites that are created. This means that the model can be 'extended' further by defining additional sites in a wrapper around this method.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_values</code> <code>ArrayLike</code> <p>Names of the keyword arguments should match the labels of the <code>ParameterNode</code>s, and their values should be the values of those parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, ArrayLike]</code> <p>Mapping of non-<code>ParameterNode</code> <code>Node</code> labels to the site objects created for these nodes.</p> Source code in <code>src/causalprog/graph/graph.py</code> <pre><code>def model(self, **parameter_values: npt.ArrayLike) -&gt; dict[str, npt.ArrayLike]:\n    \"\"\"\n    Model corresponding to the `Graph`'s structure.\n\n    The model created takes values of the nodes that are parameter as keyword\n    arguments. Names of the keyword arguments should match the labels of the\n    `ParameterNode`s, and their values should be the values of those parameters.\n\n    The method returns a dictionary recording the mode sites that are created.\n    This means that the model can be 'extended' further by defining additional\n    sites in a wrapper around this method.\n\n    Args:\n        parameter_values: Names of the keyword arguments should match the labels\n            of the `ParameterNode`s, and their values should be the values of those\n            parameters.\n\n    Returns:\n        Mapping of non-`ParameterNode` `Node` labels to the site objects created\n            for these nodes.\n\n    \"\"\"\n    # Confirm that all `ParameterNode`s have been assigned a value.\n    for node in self.parameter_nodes:\n        if node.label not in parameter_values:\n            msg = f\"ParameterNode '{node.label}' not assigned\"\n            raise KeyError(msg)\n\n    # Build model sequentially, using the node_order to inform the\n    # construction process.\n    node_record: dict[str, npt.ArrayLike] = {}\n    for node in self.ordered_dist_nodes:\n        node_record[node.label] = node.create_model_site(\n            **parameter_values,  # All nodes require knowledge of the parameters\n            **node_record,  # and any dependent nodes we have already visited\n        )\n\n    return node_record\n</code></pre>"},{"location":"api/#causalprog.graph.graph.Graph.roots_down_to_outcome","title":"<code>roots_down_to_outcome(outcome_node_label)</code>","text":"<p>Get ordered list of nodes that outcome depends on.</p> <p>Nodes are ordered so that each node appears after its dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>outcome_node_label</code> <code>str</code> <p>The label of the outcome node</p> required <p>Returns:</p> Type Description <code>tuple[Node, ...]</code> <p>A list of the nodes, ordered from root nodes to the outcome Node.</p> Source code in <code>src/causalprog/graph/graph.py</code> <pre><code>def roots_down_to_outcome(\n    self,\n    outcome_node_label: str,\n) -&gt; tuple[Node, ...]:\n    \"\"\"\n    Get ordered list of nodes that outcome depends on.\n\n    Nodes are ordered so that each node appears after its dependencies.\n\n    Args:\n        outcome_node_label: The label of the outcome node\n\n    Returns:\n        A list of the nodes, ordered from root nodes to the outcome Node.\n\n    \"\"\"\n    outcome = self.get_node(outcome_node_label)\n    ancestors = nx.ancestors(self._graph, outcome)\n    return tuple(\n        node for node in self.ordered_nodes if node == outcome or node in ancestors\n    )\n</code></pre>"},{"location":"api/#causalprog.graph.node","title":"<code>node</code>","text":"<p>Graph nodes.</p>"},{"location":"api/#causalprog.graph.node.base","title":"<code>base</code>","text":"<p>Base graph node.</p>"},{"location":"api/#causalprog.graph.node.base.Node","title":"<code>Node</code>","text":"<p>               Bases: <code>Labelled</code></p> <p>An abstract node in a graph.</p> Source code in <code>src/causalprog/graph/node/base.py</code> <pre><code>class Node(Labelled):\n    \"\"\"An abstract node in a graph.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        label: str,\n        is_parameter: bool = False,\n        is_distribution: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialise.\n\n        Parameters (equivalently `ParameterNode`s) represent Nodes that do not have\n        random variables attached. Instead, these nodes represent values that are passed\n        to nodes that _do_ have distributions attached, and the value of the \"parameter\"\n        node is used as a fixed value when constructing the dependent node's\n        distribution. The set of parameter nodes is the collection of \"parameter\"s over\n        which one should want to optimise the causal estimand (subject to any\n        constraints), and as such the value that a \"parameter node\" passes to its\n        dependent nodes will vary as the optimiser runs and explores the solution space.\n\n        Note that a \"constant parameter\" is distinct from a \"parameter\" in the sense\n        that a constant parameter is _not_ added to the collection of parameters over\n        which we will want to optimise (it is a hard-coded, fixed value).\n\n        Distributions (equivalently `DistributionNode`s) are Nodes that represent\n        random variables described by probability distributions.\n\n        Args:\n            label: A unique label to identify the node\n            is_parameter: Is the node a parameter?\n            is_distribution: Is the node a distribution?\n\n        \"\"\"\n        super().__init__(label=label)\n        self._is_parameter = is_parameter\n        self._is_distribution = is_distribution\n\n    @abstractmethod\n    def sample(\n        self,\n        parameter_values: dict[str, float],\n        sampled_dependencies: dict[str, npt.NDArray[float]],\n        samples: int,\n        *,\n        rng_key: jax.Array,\n    ) -&gt; float:\n        \"\"\"\n        Sample a value from the node.\n\n        Args:\n            parameter_values: Values to be taken by parameters\n            sampled_dependencies: Values taken by dependencies of this node\n            samples: Number of samples\n            rng_key: Random key\n\n        Returns:\n            Sample value of this node\n\n        \"\"\"\n\n    @abstractmethod\n    def copy(self) -&gt; Node:\n        \"\"\"\n        Make a copy of a node.\n\n        Some inner objects stored inside the node may not be copied when this is called.\n        Modifying some inner objects of a copy made using this may affect the original\n        node.\n\n        Returns:\n            A copy of the node\n\n        \"\"\"\n\n    @property\n    def is_parameter(self) -&gt; bool:\n        \"\"\"\n        Identify if the node is an parameter.\n\n        Returns:\n            True if the node is an parameter\n\n        \"\"\"\n        return self._is_parameter\n\n    @property\n    def is_distribution(self) -&gt; bool:\n        \"\"\"\n        Identify if the node is an distribution.\n\n        Returns:\n            True if the node is an distribution\n\n        \"\"\"\n        return self._is_distribution\n\n    @property\n    @abstractmethod\n    def constant_parameters(self) -&gt; dict[str, float]:\n        \"\"\"\n        Named constants that this node depends on.\n\n        Returns:\n            A dictionary of the constant parameter names (keys) and their corresponding\n            values\n\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def parameters(self) -&gt; dict[str, str]:\n        \"\"\"\n        Mapping of distribution parameter names to the nodes they are represented by.\n\n        Returns:\n            Mapping of distribution parameters (keys) to the corresponding label of the\n            node that represents this parameter (value).\n\n        \"\"\"\n</code></pre>"},{"location":"api/#causalprog.graph.node.base.Node.constant_parameters","title":"<code>constant_parameters</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Named constants that this node depends on.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dictionary of the constant parameter names (keys) and their corresponding</p> <code>dict[str, float]</code> <p>values</p>"},{"location":"api/#causalprog.graph.node.base.Node.is_distribution","title":"<code>is_distribution</code>  <code>property</code>","text":"<p>Identify if the node is an distribution.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the node is an distribution</p>"},{"location":"api/#causalprog.graph.node.base.Node.is_parameter","title":"<code>is_parameter</code>  <code>property</code>","text":"<p>Identify if the node is an parameter.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the node is an parameter</p>"},{"location":"api/#causalprog.graph.node.base.Node.parameters","title":"<code>parameters</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Mapping of distribution parameter names to the nodes they are represented by.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Mapping of distribution parameters (keys) to the corresponding label of the</p> <code>dict[str, str]</code> <p>node that represents this parameter (value).</p>"},{"location":"api/#causalprog.graph.node.base.Node.__init__","title":"<code>__init__(*, label, is_parameter=False, is_distribution=False)</code>","text":"<p>Initialise.</p> <p>Parameters (equivalently <code>ParameterNode</code>s) represent Nodes that do not have random variables attached. Instead, these nodes represent values that are passed to nodes that do have distributions attached, and the value of the \"parameter\" node is used as a fixed value when constructing the dependent node's distribution. The set of parameter nodes is the collection of \"parameter\"s over which one should want to optimise the causal estimand (subject to any constraints), and as such the value that a \"parameter node\" passes to its dependent nodes will vary as the optimiser runs and explores the solution space.</p> <p>Note that a \"constant parameter\" is distinct from a \"parameter\" in the sense that a constant parameter is not added to the collection of parameters over which we will want to optimise (it is a hard-coded, fixed value).</p> <p>Distributions (equivalently <code>DistributionNode</code>s) are Nodes that represent random variables described by probability distributions.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>A unique label to identify the node</p> required <code>is_parameter</code> <code>bool</code> <p>Is the node a parameter?</p> <code>False</code> <code>is_distribution</code> <code>bool</code> <p>Is the node a distribution?</p> <code>False</code> Source code in <code>src/causalprog/graph/node/base.py</code> <pre><code>def __init__(\n    self,\n    *,\n    label: str,\n    is_parameter: bool = False,\n    is_distribution: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialise.\n\n    Parameters (equivalently `ParameterNode`s) represent Nodes that do not have\n    random variables attached. Instead, these nodes represent values that are passed\n    to nodes that _do_ have distributions attached, and the value of the \"parameter\"\n    node is used as a fixed value when constructing the dependent node's\n    distribution. The set of parameter nodes is the collection of \"parameter\"s over\n    which one should want to optimise the causal estimand (subject to any\n    constraints), and as such the value that a \"parameter node\" passes to its\n    dependent nodes will vary as the optimiser runs and explores the solution space.\n\n    Note that a \"constant parameter\" is distinct from a \"parameter\" in the sense\n    that a constant parameter is _not_ added to the collection of parameters over\n    which we will want to optimise (it is a hard-coded, fixed value).\n\n    Distributions (equivalently `DistributionNode`s) are Nodes that represent\n    random variables described by probability distributions.\n\n    Args:\n        label: A unique label to identify the node\n        is_parameter: Is the node a parameter?\n        is_distribution: Is the node a distribution?\n\n    \"\"\"\n    super().__init__(label=label)\n    self._is_parameter = is_parameter\n    self._is_distribution = is_distribution\n</code></pre>"},{"location":"api/#causalprog.graph.node.base.Node.copy","title":"<code>copy()</code>  <code>abstractmethod</code>","text":"<p>Make a copy of a node.</p> <p>Some inner objects stored inside the node may not be copied when this is called. Modifying some inner objects of a copy made using this may affect the original node.</p> <p>Returns:</p> Type Description <code>Node</code> <p>A copy of the node</p> Source code in <code>src/causalprog/graph/node/base.py</code> <pre><code>@abstractmethod\ndef copy(self) -&gt; Node:\n    \"\"\"\n    Make a copy of a node.\n\n    Some inner objects stored inside the node may not be copied when this is called.\n    Modifying some inner objects of a copy made using this may affect the original\n    node.\n\n    Returns:\n        A copy of the node\n\n    \"\"\"\n</code></pre>"},{"location":"api/#causalprog.graph.node.base.Node.sample","title":"<code>sample(parameter_values, sampled_dependencies, samples, *, rng_key)</code>  <code>abstractmethod</code>","text":"<p>Sample a value from the node.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_values</code> <code>dict[str, float]</code> <p>Values to be taken by parameters</p> required <code>sampled_dependencies</code> <code>dict[str, NDArray[float]]</code> <p>Values taken by dependencies of this node</p> required <code>samples</code> <code>int</code> <p>Number of samples</p> required <code>rng_key</code> <code>Array</code> <p>Random key</p> required <p>Returns:</p> Type Description <code>float</code> <p>Sample value of this node</p> Source code in <code>src/causalprog/graph/node/base.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    parameter_values: dict[str, float],\n    sampled_dependencies: dict[str, npt.NDArray[float]],\n    samples: int,\n    *,\n    rng_key: jax.Array,\n) -&gt; float:\n    \"\"\"\n    Sample a value from the node.\n\n    Args:\n        parameter_values: Values to be taken by parameters\n        sampled_dependencies: Values taken by dependencies of this node\n        samples: Number of samples\n        rng_key: Random key\n\n    Returns:\n        Sample value of this node\n\n    \"\"\"\n</code></pre>"},{"location":"api/#causalprog.graph.node.distribution","title":"<code>distribution</code>","text":"<p>Graph nodes representing distributions.</p>"},{"location":"api/#causalprog.graph.node.distribution.DistributionNode","title":"<code>DistributionNode</code>","text":"<p>               Bases: <code>Node</code></p> <p>A node containing a distribution.</p> Source code in <code>src/causalprog/graph/node/distribution.py</code> <pre><code>class DistributionNode(Node):\n    \"\"\"A node containing a distribution.\"\"\"\n\n    def __init__(\n        self,\n        distribution: type,\n        *,\n        label: str,\n        parameters: dict[str, str] | None = None,\n        constant_parameters: dict[str, float] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialise.\n\n        Args:\n            distribution: The distribution\n            label: A unique label to identify the node\n            parameters: A dictionary of parameters\n            constant_parameters: A dictionary of constant parameters\n\n        \"\"\"\n        self._dist = distribution\n        self._constant_parameters = constant_parameters if constant_parameters else {}\n        self._parameters = parameters if parameters else {}\n        super().__init__(label=label, is_distribution=True)\n\n    @override\n    def sample(\n        self,\n        parameter_values: dict[str, float],\n        sampled_dependencies: dict[str, npt.NDArray[float]],\n        samples: int,\n        *,\n        rng_key: jax.Array,\n    ) -&gt; npt.NDArray[float]:\n        d = self._dist(\n            # Pass in node values derived from construction so far\n            **{\n                native_name: sampled_dependencies[node_name]\n                for native_name, node_name in self.parameters.items()\n            },\n            # Pass in any constant parameters this node sets\n            **self.constant_parameters,\n        )\n        return numpyro.sample(\n            self.label,\n            d,\n            rng_key=rng_key,\n            sample_shape=(samples,) if d.batch_shape == () and samples &gt; 1 else (),\n        )\n\n    @override\n    def copy(self) -&gt; Node:\n        return DistributionNode(\n            self._dist,\n            label=self.label,\n            parameters=dict(self._parameters),\n            constant_parameters=dict(self._constant_parameters.items()),\n        )\n\n    @override\n    def __repr__(self) -&gt; str:\n        r = f'DistributionNode({self._dist.__name__}, label=\"{self.label}\"'\n        if len(self._parameters) &gt; 0:\n            r += f\", parameters={self._parameters}\"\n        if len(self._constant_parameters) &gt; 0:\n            r += f\", constant_parameters={self._constant_parameters}\"\n        return r\n\n    @override\n    @property\n    def constant_parameters(self) -&gt; dict[str, float]:\n        return self._constant_parameters\n\n    @override\n    @property\n    def parameters(self) -&gt; dict[str, str]:\n        return self._parameters\n\n    def create_model_site(self, **dependent_nodes: jax.Array) -&gt; npt.ArrayLike:\n        \"\"\"\n        Create a model site for the (conditional) distribution attached to this node.\n\n        `dependent_nodes` should contain keyword arguments mapping dependent node names\n        to the values that those nodes are taking (`ParameterNode`s), or the sampling\n        object for those nodes (`DistributionNode`s). These are passed to\n        `self._dist` as keyword arguments to construct the sample-able object\n        representing this node.\n        \"\"\"\n        return numpyro.sample(\n            self.label,\n            self._dist(\n                # Pass in node values derived from construction so far\n                **{\n                    native_name: dependent_nodes[node_name]\n                    for native_name, node_name in self.parameters.items()\n                },\n                # Pass in any constant parameters this node sets\n                **self.constant_parameters,\n            ),\n        )\n</code></pre>"},{"location":"api/#causalprog.graph.node.distribution.DistributionNode.__init__","title":"<code>__init__(distribution, *, label, parameters=None, constant_parameters=None)</code>","text":"<p>Initialise.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>type</code> <p>The distribution</p> required <code>label</code> <code>str</code> <p>A unique label to identify the node</p> required <code>parameters</code> <code>dict[str, str] | None</code> <p>A dictionary of parameters</p> <code>None</code> <code>constant_parameters</code> <code>dict[str, float] | None</code> <p>A dictionary of constant parameters</p> <code>None</code> Source code in <code>src/causalprog/graph/node/distribution.py</code> <pre><code>def __init__(\n    self,\n    distribution: type,\n    *,\n    label: str,\n    parameters: dict[str, str] | None = None,\n    constant_parameters: dict[str, float] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialise.\n\n    Args:\n        distribution: The distribution\n        label: A unique label to identify the node\n        parameters: A dictionary of parameters\n        constant_parameters: A dictionary of constant parameters\n\n    \"\"\"\n    self._dist = distribution\n    self._constant_parameters = constant_parameters if constant_parameters else {}\n    self._parameters = parameters if parameters else {}\n    super().__init__(label=label, is_distribution=True)\n</code></pre>"},{"location":"api/#causalprog.graph.node.distribution.DistributionNode.create_model_site","title":"<code>create_model_site(**dependent_nodes)</code>","text":"<p>Create a model site for the (conditional) distribution attached to this node.</p> <p><code>dependent_nodes</code> should contain keyword arguments mapping dependent node names to the values that those nodes are taking (<code>ParameterNode</code>s), or the sampling object for those nodes (<code>DistributionNode</code>s). These are passed to <code>self._dist</code> as keyword arguments to construct the sample-able object representing this node.</p> Source code in <code>src/causalprog/graph/node/distribution.py</code> <pre><code>def create_model_site(self, **dependent_nodes: jax.Array) -&gt; npt.ArrayLike:\n    \"\"\"\n    Create a model site for the (conditional) distribution attached to this node.\n\n    `dependent_nodes` should contain keyword arguments mapping dependent node names\n    to the values that those nodes are taking (`ParameterNode`s), or the sampling\n    object for those nodes (`DistributionNode`s). These are passed to\n    `self._dist` as keyword arguments to construct the sample-able object\n    representing this node.\n    \"\"\"\n    return numpyro.sample(\n        self.label,\n        self._dist(\n            # Pass in node values derived from construction so far\n            **{\n                native_name: dependent_nodes[node_name]\n                for native_name, node_name in self.parameters.items()\n            },\n            # Pass in any constant parameters this node sets\n            **self.constant_parameters,\n        ),\n    )\n</code></pre>"},{"location":"api/#causalprog.graph.node.parameter","title":"<code>parameter</code>","text":"<p>Graph nodes representing parameters.</p>"},{"location":"api/#causalprog.graph.node.parameter.ParameterNode","title":"<code>ParameterNode</code>","text":"<p>               Bases: <code>Node</code></p> <p>A node containing a parameter.</p> <p><code>ParameterNode</code>s differ from <code>DistributionNode</code>s in that they do not have an attached distribution, but rather represent a parameter that contributes to the shape of one (or more) <code>DistributionNode</code>s.</p> <p>The collection of parameters described by <code>ParameterNode</code>s forms the set of variables that will be optimised over in the corresponding <code>CausalProblem</code>.</p> <p><code>ParameterNode</code>s should not be used to encode constant values used by <code>DistributionNode</code>s. Such constant values should be given to the necessary <code>DistributionNode</code>s directly as <code>constant_parameters</code>.</p> Source code in <code>src/causalprog/graph/node/parameter.py</code> <pre><code>class ParameterNode(Node):\n    \"\"\"\n    A node containing a parameter.\n\n    `ParameterNode`s differ from `DistributionNode`s in that they do not have an\n    attached distribution, but rather represent a parameter that contributes\n    to the shape of one (or more) `DistributionNode`s.\n\n    The collection of parameters described by `ParameterNode`s forms the set of\n    variables that will be optimised over in the corresponding `CausalProblem`.\n\n    `ParameterNode`s should not be used to encode constant values used by\n    `DistributionNode`s. Such constant values should be given to the necessary\n    `DistributionNode`s directly as `constant_parameters`.\n    \"\"\"\n\n    def __init__(self, *, label: str) -&gt; None:\n        \"\"\"\n        Initialise.\n\n        Args:\n            label: A unique label to identify the node\n\n        \"\"\"\n        super().__init__(label=label, is_parameter=True)\n\n    @override\n    def sample(\n        self,\n        parameter_values: dict[str, float],\n        sampled_dependencies: dict[str, npt.ArrayLike],\n        samples: int,\n        *,\n        rng_key: jax.Array,\n    ) -&gt; npt.ArrayLike:\n        if self.label not in parameter_values:\n            msg = f\"Missing input for parameter node: {self.label}.\"\n            raise ValueError(msg)\n        return jnp.full(samples, parameter_values[self.label])\n\n    @override\n    def copy(self) -&gt; Node:\n        return ParameterNode(label=self.label)\n\n    @override\n    def __repr__(self) -&gt; str:\n        return f'ParameterNode(label=\"{self.label}\")'\n\n    @override\n    @property\n    def constant_parameters(self) -&gt; dict[str, float]:\n        return {}\n\n    @override\n    @property\n    def parameters(self) -&gt; dict[str, str]:\n        return {}\n</code></pre>"},{"location":"api/#causalprog.graph.node.parameter.ParameterNode.__init__","title":"<code>__init__(*, label)</code>","text":"<p>Initialise.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>A unique label to identify the node</p> required Source code in <code>src/causalprog/graph/node/parameter.py</code> <pre><code>def __init__(self, *, label: str) -&gt; None:\n    \"\"\"\n    Initialise.\n\n    Args:\n        label: A unique label to identify the node\n\n    \"\"\"\n    super().__init__(label=label, is_parameter=True)\n</code></pre>"},{"location":"api/#causalprog.solvers","title":"<code>solvers</code>","text":"<p>Solvers for Causal Problems.</p>"},{"location":"api/#causalprog.solvers.sgd","title":"<code>sgd</code>","text":"<p>Minimisation via Stochastic Gradient Descent.</p>"},{"location":"api/#causalprog.solvers.sgd.stochastic_gradient_descent","title":"<code>stochastic_gradient_descent(obj_fn, initial_guess, *, convergence_criteria=None, fn_args=None, fn_kwargs=None, learning_rate=0.1, maxiter=1000, optimiser=None, tolerance=1e-08)</code>","text":"<p>Minimise a function of one argument using Stochastic Gradient Descent (SGD).</p> <p>The <code>obj_fn</code> provided will be minimised over its first argument. If you wish to minimise a function over a different argument, or multiple arguments, wrap it in a suitable <code>lambda</code> expression that has the correct call signature. For example, to minimise a function <code>f(x, y, z)</code> over <code>y</code> and <code>z</code>, use <code>g = lambda yz, x: f(x, yz[0], yz[1])</code>, and pass <code>g</code> in as <code>obj_fn</code>. Note that you will also need to provide a constant value for <code>x</code> via <code>fn_args</code> or <code>fn_kwargs</code>.</p> <p>The <code>fn_args</code> and <code>fn_kwargs</code> keys can be used to supply additional parameters that need to be passed to <code>obj_fn</code>, but which should be held constant.</p> <p>SGD terminates when the <code>convergence_criteria</code> is found to be smaller than the <code>tolerance</code>. That is, when <code>convergence_criteria(objective_value, gradient_value) &lt;= tolerance</code> is found to be <code>True</code>, the algorithm considers a minimum to have been found. The default condition under which the algorithm terminates is when the norm of the gradient at the current argument value is smaller than the provided <code>tolerance</code>.</p> <p>The optimiser to use can be selected by passing in a suitable <code>optax</code> optimiser via the <code>optimiser</code> command. By default, <code>optax.adams</code> is used with the supplied <code>learning_rate</code>. Providing an explicit value for <code>optimiser</code> will result in the <code>learning_rate</code> argument being ignored.</p> <p>Parameters:</p> Name Type Description Default <code>obj_fn</code> <code>Callable[[PyTree], ArrayLike]</code> <p>Function to be minimised over its first argument.</p> required <code>initial_guess</code> <code>PyTree</code> <p>Initial guess for the minimising argument.</p> required <code>convergence_criteria</code> <code>Callable[[PyTree, PyTree], ArrayLike] | None</code> <p>The quantity that will be tested against <code>tolerance</code>, to determine whether the method has converged to a minimum. It should be a <code>callable</code> that takes the current value of <code>obj_fn</code> as its 1st argument, and the current value of the gradient of <code>obj_fn</code> as its 2nd argument. The default criteria is the l2-norm of the gradient.</p> <code>None</code> <code>fn_args</code> <code>tuple | None</code> <p>Positional arguments to be passed to <code>obj_fn</code>, and held constant.</p> <code>None</code> <code>fn_kwargs</code> <code>dict | None</code> <p>Keyword arguments to be passed to <code>obj_fn</code>, and held constant.</p> <code>None</code> <code>learning_rate</code> <code>float</code> <p>Default learning rate (or step size) to use when using the default <code>optimiser</code>. No effect if <code>optimiser</code> is provided explicitly.</p> <code>0.1</code> <code>maxiter</code> <code>int</code> <p>Maximum number of iterations to perform. An error will be reported if this number of iterations is exceeded.</p> <code>1000</code> <code>optimiser</code> <code>GradientTransformationExtraArgs | None</code> <p>The <code>optax</code> optimiser to use during the update step.</p> <code>None</code> <code>tolerance</code> <code>float</code> <p><code>tolerance</code> used when determining if a minimum has been found.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>SolverResult</code> <p>Minimising argument of <code>obj_fn</code>.</p> <code>SolverResult</code> <p>Value of <code>obj_fn</code> at the minimum.</p> <code>SolverResult</code> <p>Gradient of <code>obj_fn</code> at the minimum.</p> <code>SolverResult</code> <p>Number of iterations performed.</p> Source code in <code>src/causalprog/solvers/sgd.py</code> <pre><code>def stochastic_gradient_descent(\n    obj_fn: Callable[[PyTree], npt.ArrayLike],\n    initial_guess: PyTree,\n    *,\n    convergence_criteria: Callable[[PyTree, PyTree], npt.ArrayLike] | None = None,\n    fn_args: tuple | None = None,\n    fn_kwargs: dict | None = None,\n    learning_rate: float = 1.0e-1,\n    maxiter: int = 1000,\n    optimiser: optax.GradientTransformationExtraArgs | None = None,\n    tolerance: float = 1.0e-8,\n) -&gt; SolverResult:\n    \"\"\"\n    Minimise a function of one argument using Stochastic Gradient Descent (SGD).\n\n    The `obj_fn` provided will be minimised over its first argument. If you wish to\n    minimise a function over a different argument, or multiple arguments, wrap it in a\n    suitable `lambda` expression that has the correct call signature. For example, to\n    minimise a function `f(x, y, z)` over `y` and `z`, use\n    `g = lambda yz, x: f(x, yz[0], yz[1])`, and pass `g` in as `obj_fn`. Note that\n    you will also need to provide a constant value for `x` via `fn_args` or `fn_kwargs`.\n\n    The `fn_args` and `fn_kwargs` keys can be used to supply additional parameters that\n    need to be passed to `obj_fn`, but which should be held constant.\n\n    SGD terminates when the `convergence_criteria` is found to be smaller than the\n    `tolerance`. That is, when\n    `convergence_criteria(objective_value, gradient_value) &lt;= tolerance` is found to\n    be `True`, the algorithm considers a minimum to have been found. The default\n    condition under which the algorithm terminates is when the norm of the gradient\n    at the current argument value is smaller than the provided `tolerance`.\n\n    The optimiser to use can be selected by passing in a suitable `optax` optimiser\n    via the `optimiser` command. By default, `optax.adams` is used with the supplied\n    `learning_rate`. Providing an explicit value for `optimiser` will result in the\n    `learning_rate` argument being ignored.\n\n    Args:\n        obj_fn: Function to be minimised over its first argument.\n        initial_guess: Initial guess for the minimising argument.\n        convergence_criteria: The quantity that will be tested against `tolerance`, to\n            determine whether the method has converged to a minimum. It should be a\n            `callable` that takes the current value of `obj_fn` as its 1st argument, and\n            the current value of the gradient of `obj_fn` as its 2nd argument. The\n            default criteria is the l2-norm of the gradient.\n        fn_args: Positional arguments to be passed to `obj_fn`, and held constant.\n        fn_kwargs: Keyword arguments to be passed to `obj_fn`, and held constant.\n        learning_rate: Default learning rate (or step size) to use when using the\n            default `optimiser`. No effect if `optimiser` is provided explicitly.\n        maxiter: Maximum number of iterations to perform. An error will be reported if\n            this number of iterations is exceeded.\n        optimiser: The `optax` optimiser to use during the update step.\n        tolerance: `tolerance` used when determining if a minimum has been found.\n\n    Returns:\n        Minimising argument of `obj_fn`.\n        Value of `obj_fn` at the minimum.\n        Gradient of `obj_fn` at the minimum.\n        Number of iterations performed.\n\n    \"\"\"\n    if not fn_args:\n        fn_args = ()\n    if not fn_kwargs:\n        fn_kwargs = {}\n    if not convergence_criteria:\n        convergence_criteria = lambda _, dx: jnp.sqrt(l2_normsq(dx))  # noqa: E731\n    if not optimiser:\n        optimiser = optax.adam(learning_rate)\n\n    def objective(x: npt.ArrayLike) -&gt; npt.ArrayLike:\n        return obj_fn(x, *fn_args, **fn_kwargs)\n\n    def is_converged(x: npt.ArrayLike, dx: npt.ArrayLike) -&gt; bool:\n        return convergence_criteria(x, dx) &lt; tolerance\n\n    converged = False\n\n    opt_state = optimiser.init(initial_guess)\n    current_params = deepcopy(initial_guess)\n    gradient = jax.grad(objective)\n\n    for _ in range(maxiter + 1):\n        objective_value = objective(current_params)\n        gradient_value = gradient(current_params)\n\n        if converged := is_converged(objective_value, gradient_value):\n            break\n\n        updates, opt_state = optimiser.update(gradient_value, opt_state)\n        current_params = optax.apply_updates(current_params, updates)\n\n    iters_used = _\n    reason_msg = (\n        f\"Did not converge after {iters_used} iterations\" if not converged else \"\"\n    )\n\n    return SolverResult(\n        fn_args=current_params,\n        grad_val=gradient_value,\n        iters=iters_used,\n        maxiter=maxiter,\n        obj_val=objective_value,\n        reason=reason_msg,\n        successful=converged,\n    )\n</code></pre>"},{"location":"api/#causalprog.solvers.solver_result","title":"<code>solver_result</code>","text":"<p>Container class for outputs from solver methods.</p>"},{"location":"api/#causalprog.solvers.solver_result.SolverResult","title":"<code>SolverResult</code>  <code>dataclass</code>","text":"<p>Container class for outputs from solver methods.</p> <p>Instances of this class provide a container for useful information that comes out of running one of the solver methods on a causal problem.</p> <p>Attributes:</p> Name Type Description <code>fn_args</code> <code>PyTree</code> <p>Argument to the objective function at final iteration (the solution, if <code>successful is</code>True`).</p> <code>grad_val</code> <code>PyTree</code> <p>Value of the gradient of the objective function at the <code>fn_args</code>.</p> <code>iters</code> <code>int</code> <p>Number of iterations performed.</p> <code>maxiter</code> <code>int</code> <p>Maximum number of iterations the solver was permitted to perform.</p> <code>obj_val</code> <code>ArrayLike</code> <p>Value of the objective function at <code>fn_args</code>.</p> <code>reason</code> <code>str</code> <p>Human-readable string explaining success or reasons for solver failure.</p> <code>successful</code> <code>bool</code> <p><code>True</code> if solver converged, in which case <code>fn_args</code> is the argument to the objective function at the solution of the problem being solved. <code>False</code> otherwise.</p> Source code in <code>src/causalprog/solvers/solver_result.py</code> <pre><code>@dataclass(frozen=True)\nclass SolverResult:\n    \"\"\"\n    Container class for outputs from solver methods.\n\n    Instances of this class provide a container for useful information that\n    comes out of running one of the solver methods on a causal problem.\n\n    Attributes:\n        fn_args: Argument to the objective function at final iteration (the solution,\n            if `successful is `True`).\n        grad_val: Value of the gradient of the objective function at the `fn_args`.\n        iters: Number of iterations performed.\n        maxiter: Maximum number of iterations the solver was permitted to perform.\n        obj_val: Value of the objective function at `fn_args`.\n        reason: Human-readable string explaining success or reasons for solver failure.\n        successful: `True` if solver converged, in which case `fn_args` is the\n            argument to the objective function at the solution of the problem being\n            solved. `False` otherwise.\n\n    \"\"\"\n\n    fn_args: PyTree\n    grad_val: PyTree\n    iters: int\n    maxiter: int\n    obj_val: npt.ArrayLike\n    reason: str\n    successful: bool\n</code></pre>"},{"location":"api/#causalprog.utils","title":"<code>utils</code>","text":"<p>Utility classes and methods.</p>"},{"location":"api/#causalprog.utils.norms","title":"<code>norms</code>","text":"<p>Misc collection of norm-like functions for PyTree structures.</p>"},{"location":"api/#causalprog.utils.norms.l2_normsq","title":"<code>l2_normsq(x)</code>","text":"<p>Square of the l2-norm of a PyTree.</p> <p>This is effectively \"sum(elements**2 in leaf for leaf in x)\".</p> Source code in <code>src/causalprog/utils/norms.py</code> <pre><code>def l2_normsq(x: PyTree) -&gt; npt.ArrayLike:\n    \"\"\"\n    Square of the l2-norm of a PyTree.\n\n    This is effectively \"sum(elements**2 in leaf for leaf in x)\".\n    \"\"\"\n    leaves, _ = jax.tree_util.tree_flatten(x)\n    return sum(jax.numpy.sum(leaf**2) for leaf in leaves)\n</code></pre>"},{"location":"api/#causalprog.utils.translator","title":"<code>translator</code>","text":"<p>Helper class to keep the codebase backend-agnostic.</p> <p>Our frontend (or user-facing) classes each use a syntax that applies across the package codebase. By contrast, the various backends that we want to support will have different syntaxes and call signatures for the functions that we want to support. As such, we need a helper class that can store this \"translation\" information, allowing the user to interact with the package in a standard way but also allowing them to choose their own backend if desired.</p>"},{"location":"api/#causalprog.utils.translator.Translator","title":"<code>Translator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Maps syntax of a backend function to our frontend syntax.</p> <p>Different backends have different syntax for drawing samples from the distributions they support. In order to map these different syntaxes to our backend-agnostic framework, we need a container class to map the names we have chosen for our frontend methods to those used by their corresponding backend method.</p> <p>A <code>Translator</code> allows us to identify whether a user-provided backend object is compatible with one of our frontend wrapper classes (and thus, call signatures). It also allows users to write their own translators for any custom backends that we do not explicitly support.</p> <p>The use case for a <code>Translator</code> is as follows. Suppose that we have a frontend class <code>C</code> that needs to provide a method <code>do_something</code>. <code>C</code> stores a reference to a backend object <code>obj</code> that can provide the functionality of <code>do_something</code> via one of its methods, <code>obj.backend_method</code>. However, there is no guarantee that the signature of <code>do_something</code> maps identically to that of <code>obj.backend_method</code>. A <code>Translator</code> allows us to encode a mapping of <code>obj.backend_method</code>s arguments to those of <code>do_something</code>.</p> Source code in <code>src/causalprog/utils/translator.py</code> <pre><code>class Translator(ABC):\n    \"\"\"\n    Maps syntax of a backend function to our frontend syntax.\n\n    Different backends have different syntax for drawing samples from the distributions\n    they support. In order to map these different syntaxes to our backend-agnostic\n    framework, we need a container class to map the names we have chosen for our\n    frontend methods to those used by their corresponding backend method.\n\n    A ``Translator`` allows us to identify whether a user-provided backend object is\n    compatible with one of our frontend wrapper classes (and thus, call signatures). It\n    also allows users to write their own translators for any custom backends that we do\n    not explicitly support.\n\n    The use case for a ``Translator`` is as follows. Suppose that we have a frontend\n    class ``C`` that needs to provide a method ``do_something``. ``C`` stores a\n    reference to a backend object ``obj`` that can provide the functionality of\n    ``do_something`` via one of its methods, ``obj.backend_method``. However, there is\n    no guarantee that the signature of ``do_something`` maps identically to that of\n    ``obj.backend_method``. A ``Translator`` allows us to encode a mapping of\n    ``obj.backend_method``s arguments to those of ``do_something``.\n    \"\"\"\n\n    backend_method: str\n    corresponding_backend_arg: dict[str, str]\n\n    @property\n    @abstractmethod\n    def _frontend_method(self) -&gt; str:\n        \"\"\"Name of the frontend method that the backend is to be translated into.\"\"\"\n\n    @property\n    @abstractmethod\n    def compulsory_frontend_args(self) -&gt; set[str]:\n        \"\"\"Arguments that are required by the frontend function.\"\"\"\n\n    @property\n    def compulsory_backend_args(self) -&gt; set[str]:\n        \"\"\"Arguments that are required to be taken by the backend function.\"\"\"\n        return {\n            self.corresponding_backend_arg[arg_name]\n            for arg_name in self.compulsory_frontend_args\n        }\n\n    def __init__(\n        self, backend_method: str | None = None, **front_args_to_back_args: str\n    ) -&gt; None:\n        \"\"\"\n        Create a new Translator.\n\n        Args:\n            backend_method (str): Name of the backend method that the instance\n                translates.\n            **front_args_to_back_args (str): Mapping of frontend argument names to the\n                corresponding backend argument names.\n\n        \"\"\"\n        # Assume backend name is identical to frontend name if not provided explicitly\n        self.backend_method = (\n            backend_method if backend_method else self._frontend_method\n        )\n\n        # This should really be immutable after we fill defaults!\n        self.corresponding_backend_arg = dict(front_args_to_back_args)\n        # Assume compulsory frontend args that are not given translations\n        # retain their name in the backend.\n        for arg in self.compulsory_frontend_args:\n            if arg not in self.corresponding_backend_arg:\n                self.corresponding_backend_arg[arg] = arg\n\n    def translate_args(self, **kwargs: Any) -&gt; dict[str, Any]:  # noqa: ANN401\n        \"\"\"\n        Translate frontend arguments (with values) to backend arguments.\n\n        Essentially transforms frontend keyword arguments into their backend keyword\n        arguments, preserving the value assigned to each argument.\n        \"\"\"\n        return {\n            self.corresponding_backend_arg[arg_name]: arg_value\n            for arg_name, arg_value in kwargs.items()\n        }\n\n    def validate_compatible(self, obj: object) -&gt; None:\n        \"\"\"\n        Determine if ``obj`` provides a compatible backend method.\n\n        ``obj`` must provide a callable whose name matches ``self.backend_method``,\n        and the callable referenced must take arguments matching the names specified in\n        ``self.compulsory_backend_args``.\n\n        Args:\n            obj (object): Object to check possesses a method that can be translated into\n                frontend syntax.\n\n        \"\"\"\n        # Check that obj does provide a method of matching name\n        if not hasattr(obj, self.backend_method):\n            msg = f\"{obj} has no method '{self.backend_method}'.\"\n            raise AttributeError(msg)\n        if not callable(getattr(obj, self.backend_method)):\n            msg = f\"'{self.backend_method}' attribute of {obj} is not callable.\"\n            raise TypeError(msg)\n\n        # Check that this method will be callable with the information given.\n        method_params = inspect.signature(getattr(obj, self.backend_method)).parameters\n        # The arguments that will be passed are actually taken by the method.\n        for compulsory_arg in self.compulsory_backend_args:\n            if compulsory_arg not in method_params:\n                msg = (\n                    f\"'{self.backend_method}' does not \"\n                    f\"take argument '{compulsory_arg}'.\"\n                )\n                raise TypeError(msg)\n        # The method does not _require_ any additional arguments\n        method_requires = {\n            name for name, p in method_params.items() if p.default is p.empty\n        }\n        if not method_requires.issubset(self.compulsory_backend_args):\n            args_not_accounted_for = method_requires - self.compulsory_backend_args\n            raise TypeError(\n                f\"'{self.backend_method}' not provided compulsory arguments \"\n                \"(missing \" + \", \".join(args_not_accounted_for) + \")\"\n            )\n</code></pre>"},{"location":"api/#causalprog.utils.translator.Translator.compulsory_backend_args","title":"<code>compulsory_backend_args</code>  <code>property</code>","text":"<p>Arguments that are required to be taken by the backend function.</p>"},{"location":"api/#causalprog.utils.translator.Translator.compulsory_frontend_args","title":"<code>compulsory_frontend_args</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Arguments that are required by the frontend function.</p>"},{"location":"api/#causalprog.utils.translator.Translator.__init__","title":"<code>__init__(backend_method=None, **front_args_to_back_args)</code>","text":"<p>Create a new Translator.</p> <p>Parameters:</p> Name Type Description Default <code>backend_method</code> <code>str</code> <p>Name of the backend method that the instance translates.</p> <code>None</code> <code>**front_args_to_back_args</code> <code>str</code> <p>Mapping of frontend argument names to the corresponding backend argument names.</p> <code>{}</code> Source code in <code>src/causalprog/utils/translator.py</code> <pre><code>def __init__(\n    self, backend_method: str | None = None, **front_args_to_back_args: str\n) -&gt; None:\n    \"\"\"\n    Create a new Translator.\n\n    Args:\n        backend_method (str): Name of the backend method that the instance\n            translates.\n        **front_args_to_back_args (str): Mapping of frontend argument names to the\n            corresponding backend argument names.\n\n    \"\"\"\n    # Assume backend name is identical to frontend name if not provided explicitly\n    self.backend_method = (\n        backend_method if backend_method else self._frontend_method\n    )\n\n    # This should really be immutable after we fill defaults!\n    self.corresponding_backend_arg = dict(front_args_to_back_args)\n    # Assume compulsory frontend args that are not given translations\n    # retain their name in the backend.\n    for arg in self.compulsory_frontend_args:\n        if arg not in self.corresponding_backend_arg:\n            self.corresponding_backend_arg[arg] = arg\n</code></pre>"},{"location":"api/#causalprog.utils.translator.Translator.translate_args","title":"<code>translate_args(**kwargs)</code>","text":"<p>Translate frontend arguments (with values) to backend arguments.</p> <p>Essentially transforms frontend keyword arguments into their backend keyword arguments, preserving the value assigned to each argument.</p> Source code in <code>src/causalprog/utils/translator.py</code> <pre><code>def translate_args(self, **kwargs: Any) -&gt; dict[str, Any]:  # noqa: ANN401\n    \"\"\"\n    Translate frontend arguments (with values) to backend arguments.\n\n    Essentially transforms frontend keyword arguments into their backend keyword\n    arguments, preserving the value assigned to each argument.\n    \"\"\"\n    return {\n        self.corresponding_backend_arg[arg_name]: arg_value\n        for arg_name, arg_value in kwargs.items()\n    }\n</code></pre>"},{"location":"api/#causalprog.utils.translator.Translator.validate_compatible","title":"<code>validate_compatible(obj)</code>","text":"<p>Determine if <code>obj</code> provides a compatible backend method.</p> <p><code>obj</code> must provide a callable whose name matches <code>self.backend_method</code>, and the callable referenced must take arguments matching the names specified in <code>self.compulsory_backend_args</code>.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>Object to check possesses a method that can be translated into frontend syntax.</p> required Source code in <code>src/causalprog/utils/translator.py</code> <pre><code>def validate_compatible(self, obj: object) -&gt; None:\n    \"\"\"\n    Determine if ``obj`` provides a compatible backend method.\n\n    ``obj`` must provide a callable whose name matches ``self.backend_method``,\n    and the callable referenced must take arguments matching the names specified in\n    ``self.compulsory_backend_args``.\n\n    Args:\n        obj (object): Object to check possesses a method that can be translated into\n            frontend syntax.\n\n    \"\"\"\n    # Check that obj does provide a method of matching name\n    if not hasattr(obj, self.backend_method):\n        msg = f\"{obj} has no method '{self.backend_method}'.\"\n        raise AttributeError(msg)\n    if not callable(getattr(obj, self.backend_method)):\n        msg = f\"'{self.backend_method}' attribute of {obj} is not callable.\"\n        raise TypeError(msg)\n\n    # Check that this method will be callable with the information given.\n    method_params = inspect.signature(getattr(obj, self.backend_method)).parameters\n    # The arguments that will be passed are actually taken by the method.\n    for compulsory_arg in self.compulsory_backend_args:\n        if compulsory_arg not in method_params:\n            msg = (\n                f\"'{self.backend_method}' does not \"\n                f\"take argument '{compulsory_arg}'.\"\n            )\n            raise TypeError(msg)\n    # The method does not _require_ any additional arguments\n    method_requires = {\n        name for name, p in method_params.items() if p.default is p.empty\n    }\n    if not method_requires.issubset(self.compulsory_backend_args):\n        args_not_accounted_for = method_requires - self.compulsory_backend_args\n        raise TypeError(\n            f\"'{self.backend_method}' not provided compulsory arguments \"\n            \"(missing \" + \", \".join(args_not_accounted_for) + \")\"\n        )\n</code></pre>"},{"location":"developers/tests/","title":"Testing Suite","text":"<p><code>causalprog</code>'s test suite is written using <code>pytest</code>. The package can be installed with its developer dependencies, including <code>pytest</code>, by specifying the <code>[dev]</code> optional dependency when installing the package.</p>"},{"location":"developers/tests/#running-the-tests","title":"Running the tests","text":"<p>To run the test suite, you will need to clone the <code>causalprog</code> repository and then install <code>causalprog</code> into your developer environment with the <code>[dev]</code> optional dependencies. We recommend specifying an editable installation if you intend to make contributions to the package.</p> <pre><code>(causalprog-environment) $ git clone git@github.com:UCL/causalprog.git\n(causalprog-environment) $ cd causalprog\n(causalprog-environment) $ pip install -e .[dev]\n</code></pre> <p>You can then run the tests manually inside your developer environment from the root of the repository,</p> <pre><code>(causalprog-environment) $ pytest tests/\n</code></pre> <p>Alternatively, tests can be run across all compatible Python versions in isolated environments using <code>tox</code>. Running</p> <pre><code>(causalprog-environment) $ tox\n</code></pre> <p>in the repository root will do so.</p>"},{"location":"developers/tests/#organisation-of-the-test-suite","title":"Organisation of the test suite","text":"<p>The test suite contains a <code>fixtures</code> subdirectory, which is loaded as a <code>pytest</code> plugin when the tests are run. All <code>pytest.fixture</code> objects defined inside the <code>fixtures</code> subdirectory (and subdirectories therein) are discovered by <code>pytest</code>, and available for use by individual tests.</p> <ul> <li>Fixtures that are shared across multiple test files should be refactored into this folder, being placed into an appropriate file.   If possible, include a docstring describing what the fixture does (if it is a method or function) or the instance it defines (if it defines an instance of a class, for example a fixed <code>Graph</code> used in multiple tests).</li> <li>Fixtures that are only used by tests within a single file, should be defined in that file rather than the <code>fixtures</code> directory.</li> </ul> <p>We favour granularity for the files containing the tests themselves. Unit tests are stored starting at the same level as the <code>fixtures</code> directory. Our general guidelines for organising unit tests are:</p> <ul> <li>Use subdirectories to group files containing tests by module and class.</li> <li>For each method or function; write all its unit tests inside a single file.</li> <li>Keep to the limit of one method / function being tested per file, except in cases where it is sensible to include closely related methods / functions.</li> </ul> <p>Any integration tests should be placed into the <code>test_integration</code> subfolder. Again, this directory should contain a single file per integration test.</p>"},{"location":"developers/tests/#useful-fixtures","title":"Useful fixtures","text":"<p>Some useful fixtures that are included in the <code>fixtures</code> directory;</p> <ul> <li><code>ssed</code> and <code>rng_key</code> (<code>fixtures/general.py</code>) - sets the PRNG Key that should be used across all tests, to ensure repeatability.</li> <li><code>raises_context</code> (<code>fixtures/general.py</code>) - can be used to return a <code>pytest.raises</code> context that checks for a specific exception, including matching the error message.</li> </ul>"},{"location":"theory/mathematical-context/","title":"Causal Problems and <code>causalprog</code>","text":"<p>TL;DR, <code>causalprog</code> solves</p> \\[ \\max_{\\Theta} / \\min_{\\Theta} \\sigma(\\Theta), \\quad \\text{subject to } \\quad \\mathrm{dist}(\\phi_\\mathrm{data}, \\phi_\\mathrm{model}(\\Theta))\\leq \\epsilon, \\] <p>given</p> <ul> <li>a (parametrised) causal model \\(\\Theta\\),</li> <li>a causal estimand \\(\\sigma\\),</li> </ul> <p>and matching constraints \\(\\phi = (\\phi_j)\\), where;</p> <ul> <li>\\(\\phi_\\mathrm{data}\\) is empirically observed values of \\(\\phi\\),</li> <li>\\(\\phi_\\mathrm{model}\\) is the analytical estimate of \\(\\phi\\) given \\(\\Theta\\),</li> <li>\\(\\mathrm{dist}\\) is a non-negative valued distance function (such as a suitable norm),</li> <li>a tolerance parameter \\(\\epsilon\\).</li> </ul>"},{"location":"theory/mathematical-context/#causal-problems","title":"Causal Problems","text":"<p>In full generality, we can describe a causal problem as follows.</p> <p>Let \\(X_1, X_2, ..., X_I\\) ( \\(I\\in\\mathbb{N}\\) ) be a collection of random variables. For each \\(i\\), let \\(V_i \\subset {1, ..., i-1}\\) be the (possibly empty) collection of the indices of the random variables that \\(X_i\\) is dependant upon. Note that we are assuming (WLOG) that the random variables are indexed somewhat sequentially in terms of causality / dependency.</p> <p>The structure imposed by the \\(V_i\\) allows for the relationships between the \\(X_i\\) to be realised as a DAG (Directed Acyclic Graph). The nodes represent the random variables \\(X_i\\), and as such we use the notation \\(X_i\\) interchangeably when referring to the random variables or nodes of the associated DAG. An edge directed into \\(X_i\\) from \\(X_k\\) (where \\((k &lt; i)\\)) encodes that the distribution of \\(X_i\\) depends on \\(X_k\\).</p> <p>Let \\(D_i = \\otimes_{k\\in V_i} X_k\\) and for each \\(X_i\\). Assume there exists a function \\(f_{X_i}\\), deterministic in its arguments, and with \\(\\mathrm{dom}(f_{x_i}) = D_i\\), such that \\(X_i \\sim f_{X_i}\\). That is to say, for each \\(i\\) there is some deterministic function \\(f_{X_i}\\) such that, given realisations of \\(X_k, k\\in V_i\\), \\(f_{X_i}\\) fully describes the distribution of \\(X_i\\). We will refer to the \\(f_{X_i}\\) as the structural equation of \\(X_i\\). The (parametrised) causal model is then \\(\\Theta := \\left\\{ f_{X_i} \\right\\}_{i\\leq n}\\).</p>"},{"location":"theory/mathematical-context/#further-parametrisations-of-theta","title":"Further Parametrisations of \\(\\Theta\\)","text":"<p>A causal model \\(\\Theta\\) is parametrised by the structural equations, which themselves may be further parametrised. In such a case, it is convenient to view the parametrisation of the structural equations as the parametrisation of \\(\\Theta\\).</p> <p>For example, in equation (1), Padh et. al., the structural equations are expressed as an expansion of (fixed) basis functions \\(\\left\\{\\psi_{i, j}\\right\\}_{i\\leq I, j\\leq J}\\), \\(J\\in\\mathbb{N}\\):</p> \\[ f_{X_i} = \\sum_{j=1}^{J} \\theta_{X_i}^{(j)}\\psi_{i_j}. \\] <p>Each \\(f_{X_i}\\) is thus fully described in terms of their coefficients \\(\\theta_{X_i} := (\\theta_{X_i}^{(j)})_{j\\leq J}\\). In such a case it is suitable to directly parametrise \\(\\Theta = \\left\\{\\theta_{X_i}\\right\\}_{i\\leq I}\\) rather than in terms of \\(f_{X_i}\\).</p>"},{"location":"theory/mathematical-context/#causal-estimands","title":"Causal Estimands","text":"<p>Next, let \\(\\sigma\\) be a causal estimand of interest; that is to say, some quantity to be calculated from \\(\\Theta\\), so \\(\\sigma = \\sigma(\\Theta)\\). This could be something like the expectation or variance of one of the random variables \\(X_k\\), for example.</p> <p>For the time being, <code>causalprog</code> focuses on casual estimands that are predominantly integrals of some type. In particular, the focus is on causal estimands that are the expectations (or possibly higher moments) of one of the random variables \\(X_k\\) given some other conditions. As such, computing the value of a causal estimand will be done largely through Monte Carlo sampling to approximate these integrands. Since no assumption is made on the dimensionality of our random variables (and thus domains of the integrals), some of these integrals may require a large number of samples before giving a suitable approximation to the true value.</p>"},{"location":"theory/mathematical-context/#the-mathrmdo-operator","title":"The \\(\\mathrm{do}\\) Operator","text":"<p>One particular estimand of interest is the effect of do-ing something, described by the \\(\\mathrm{do}\\) operator. The expected value of \\(X_k\\) given that we \"do\" \\(X_l = x^*\\) is written as \\(\\mathbb{E}[ X_k \\vert \\mathrm{do}(X_l = x^*) ]\\). In general, this is different from \\(\\mathbb{E}[ X_k \\vert X_l = x^* ]\\).</p> <p>However, the \\(\\mathrm{do}\\) operator has a relatively simple-to-explain effect on \\(\\Theta\\); essentially replace (the function) \\(f_{X_l}\\) with the constant \\(x^*\\) (or the appropriate mathematical object it defines).</p> <p>In the simple case where we have a random variable \\(Y\\) which depends on \\(X\\) (which we can control or fix) and \\(U\\) (which we cannot control), we have that</p> \\[\\mathbb{E}[ Y \\vert \\mathrm{do}(X = x^*) ] = \\int f_{Y}(x^*, u) \\ \\mathrm{d}u \\\\ \\approx \\frac{1}{M} \\sum_{i=1}^M f_Y(x^*, u^{(i)}),\\] <p>with the approximation following from a Monte Carlo estimation of the integrand using samples \\(u^{(i)}\\) drawn from \\(U\\).</p>"},{"location":"theory/mathematical-context/#matching-constraints","title":"Matching Constraints","text":"<p>The matching constraints \\(\\phi\\) are observable quantities that ensure the theoretical model remains representative of our empirical observations. When bounds for causal estimands are a concern, they serve to restrict the space of admissible causal models and thus tighten the obtainable bounds.</p> <p>In general \\(\\phi = (\\phi_k)_{k\\leq K}\\), \\(K\\in\\mathbb{N}\\) is a \\(K\\)-dimensional vector of matching constraints \\(\\phi_k\\). The quantities described in each \\(\\phi_k\\) can be empirically observed to give some data vector \\(\\phi_{\\mathrm{data}}\\), and estimated from a causal model \\(\\Theta\\) to give the theoretical values of these observables \\(\\phi_{\\mathrm{model}} = \\phi_{\\mathrm{model}}(\\Theta)\\).</p> <p>A common set of \\(\\phi_k\\) are moment-matching constraints of the form</p> \\[ \\phi_{i, 0} = \\mathbb{E}[X_i], \\quad \\phi_{i,j} = \\mathbb{E}[X_i X_j], \\] <p>for the observable random variables \\(X_i\\) (note that we can map the \\((i,j)\\)-indexing to a single index \\(k\\)).</p> <p>To attempt to infer the underlying causal model from observed data \\(\\phi_{\\mathrm{data}}\\), one would have to examine the set of causal models for which \\(\\phi_{\\mathrm{data}} = \\phi_{\\mathrm{model}}\\). In practice, we are typically concerned with those causal models that are \"close to\" \\(\\phi_{\\mathrm{data}}\\), rather than exactly equal, due to measurement inaccuracies or computational limitations. As such, one may provide a suitable distance function \\(\\mathrm{dist}\\) and tolerance parameter \\(\\epsilon\\). \\(\\mathrm{dist}(\\phi_{\\mathrm{data}}, \\phi_{\\mathrm{model}})\\) is interpreted as a quantification of the difference between the observed and expected constraint values, with a value of 0 indicating equality.</p> \\[\\mathrm{dist}(\\phi_{\\mathrm{data}}, \\phi_{\\mathrm{model}}) = \\vert\\vert \\phi_{\\mathrm{data}} - \\phi_{\\mathrm{model}} \\vert\\vert^2_{L^2} \\] <p>would be such an example.</p>"},{"location":"theory/mathematical-context/#bounds-for-causal-estimands","title":"Bounds for Causal Estimands","text":"<p>Given a causal estimand \\(\\sigma\\), it is natural to ask whether we can provide bounds for \\(\\sigma\\) given some empirical observations of (observable variables of) \\(\\Theta\\).</p> <p>To obtain suitable bounds on \\(\\sigma\\), and using the notation introduced in the sections above, we must solving the following (pair of) optimization problem(s):</p> \\[ \\max_\\Theta / \\min_\\Theta \\sigma(\\Theta), \\quad \\text{subject to } \\mathrm{dist}\\left(\\phi_\\mathrm{data} - \\phi_\\mathrm{model}(\\Theta) \\right) \\leq \\epsilon, \\] <p>Solving for the minimum provides the lower bound for \\(\\sigma\\), and solving for the maximum the upper bound. The corresponding argument-min \\(\\Theta_{\\mathrm{min}}\\) (respectively argument-max \\(\\Theta_{\\mathrm{max}}\\)) are the realisable causal models (IE the causal models that are consistent with our empirical observations up to the given tolerance) that attain the bounds of \\(\\sigma\\).</p> <p>Such problems can be tackled using approaches based on Lagrangian multipliers, for example, seeking the saddle points of the augmented lagrangian</p> \\[ \\mathcal{L}(\\Theta, \\lambda) := \\sigma(\\Theta) - \\lambda \\left( \\mathrm{dist}\\left(\\phi_\\mathrm{data} - \\phi_\\mathrm{model}(\\Theta) \\right)- \\epsilon\\right), \\] <p>and then determining whether they are maxima or minima.</p>"},{"location":"theory/simple-working-example/","title":"First steps: simple working example","text":"<p>As a starting point for the package, we will focus on implementing the required functionality to solve the simplified problem detailed below. During the description, we will highlight areas that will need to be designed with further generality in mind. Once the simplified example is working and tested, we will begin relaxing the assumptions that reduced us to the simplified problem - both mathematically and programmatically - and gradually expand the scope of the package outwards, from this basis.</p>"},{"location":"theory/simple-working-example/#problem-statement","title":"Problem Statement","text":"<p>We take two random variables \\(X\\) and \\(Y\\) with the following structural equations;</p> \\[ f_X = \\mathcal{N}(\\mu_X, \\nu_X^2), \\quad f_Y = \\mathcal{N}(X, \\nu_Y^2), \\] <p>where we will assume, for the time being, that \\(\\nu_X, \\nu_Y \\in [0,\\infty)\\) are fixed values, and \\(\\mu_X\\) is a parameter in our causal model. This means that the causal model, \\(\\Theta = \\{\\mu_X\\}\\) has only one parameter for the time being.</p> <p>Our causal estimand of interest will simply be the expectation of our \"outcome\" variable \\(Y\\);</p> \\[ \\sigma = \\mathbb{E}[Y]. \\] <p>Our constraints will be the observed mean of \\(X\\), \\(\\phi = \\mathbb{E}[X]\\), and we will take our distance function to be \\(\\mathrm{dist}(\\phi, \\psi) = \\vert \\phi - \\psi \\vert\\) (essentially a 1-dimensional \\(L^2\\)-norm). For a given tolerance \\(\\epsilon\\), we thus have the following problem to solve;</p> \\[ \\max_{\\mu_X} / \\min_{\\mu_X} \\mathbb{E}[Y], \\quad\\text{subject to}\\quad \\vert \\phi_{\\mathrm{data}} - \\mathbb{E}[X] \\vert \\leq \\epsilon. \\] <p>By the structural equations, we can infer that \\(\\mathbb{E}[Y] = \\mathbb{E}[X] = \\mu_X\\), thus reaching</p> \\[ \\max_{\\mu_X} / \\min_{\\mu_X} \\mu_X, \\quad\\text{subject to}\\quad \\vert \\phi_{\\mathrm{data}} - \\mu_X \\vert \\leq \\epsilon. \\] <p>The solution to this problem is \\(\\mu_X = \\phi_\\mathrm{data} \\pm \\epsilon\\) (the positive solution corresponding to the maximisation).</p> <p>The purpose of solving this simple problem is that it will force us (as developers) to answer a number of structural questions about the package.</p>"},{"location":"theory/simple-working-example/#generalising","title":"Generalising","text":"<p>Once we have a working implementation of the above problem, we will begin generalising the problem above and expanding the functionality of the package to match. Some immediate ideas for generalisations are as follows:</p> <ul> <li>Generalise to multiple parameters. The example above could - in particular - have \\(\\nu_X\\) and \\(\\nu_Y\\) be parameters to the model too. This will require us to ensure that whatever general classes we have to represent the structural equations are broad enough to cope with arbitrarily-shaped parameters.</li> <li>A related task is to generalise the structural equations to other (non-normal) distributions first, and then on to more complex \"distributions\" like feed-forward networks.</li> <li>Generalise to multiple constraints. Immediate possibilities would be imposing constraints on the variance of \\(X\\) and/or \\(Y\\), particularly if we have already made \\(\\nu_X\\) and/or \\(\\nu_Y\\) parameters themselves (rather than fixed values).</li> <li>Generalise to arbitrary distance functions. This is likely some low-hanging fruit at first, but has the potential to be quite complex if we later want to use functionality like \"auto-diff\" to speed up solving the optimisation problem.</li> <li>Generalising to arbitrary causal estimates.</li> <li>Though not particularly general, having a method or function that applies the \\(\\mathrm{do}\\) operator would be of interest beyond just use in a causal estimand. Though for the time being, we might only want it to be applicable to root nodes.</li> </ul>"}]}