{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a Causal Problem with `causalprog`\n",
    "\n",
    "This notebook demonstrates how to use `causalprog` to compute bounds on the causal estimand for a simple causal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem Setup and Analytic Solution\n",
    "\n",
    "We will assume that we have the following random variables;\n",
    "\n",
    "\\begin{align*}\n",
    "X \\sim \\mathcal{N}(\\mu_X, 1.0),\n",
    "&\\quad \n",
    "Y \\mid X \\sim \\mathcal{N}(X, \\nu_Y)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mu_X$ and $\\nu_Y$ are the parameters for our problem.\n",
    "\n",
    "**Note:** to disambiguate between terms, `causalprog` refers to $\\mu_X$ and $\\nu_Y$ as the **model parameters**.\n",
    "Model parameters are the variables that fully define the model described in the causal problem (equivalently that parameterise the RVs that appear in the DAG).\n",
    "\n",
    "Our causal estimand $\\sigma$ is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma(\\mu_X, \\nu_Y) = \\mathbb{E}[Y],\n",
    "\\end{align*}\n",
    "\n",
    "which we can analytically compute to be $\\sigma(\\mu_X, \\nu_Y) = \\mu_X$.\n",
    "The quantities predicted by our data $\\phi_{obs}$ (which form our constraints) are given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\phi(\\mu_X, \\nu_Y) = \\mathbb{E}[X],\n",
    "\\end{align*}\n",
    "\n",
    "where again we can analytically compute that $\\phi(\\mu_X, \\nu_Y) = \\mu_X$.\n",
    "We'll also assume that we have some tolerance $\\epsilon$ in our data.\n",
    "\n",
    "Thus, we're aiming to solve the following optimisation problems;\n",
    "\n",
    "\\begin{align*}\n",
    "\\max/\\min_{\\mu_X, \\nu_Y} \\sigma\n",
    "&= \\max/\\min_{\\mu_X, \\nu_Y} \\mu_X, \\\\\n",
    "\\text{subject to } \\quad \\vert \\phi(\\mu_X, \\nu_Y) - \\phi_{obs} \\vert &= \\vert \\mu_X - \\phi_{obs} \\vert \\leq \\epsilon.\n",
    "\\end{align*}\n",
    "\n",
    "The solution to this is $\\mu_X^{*} = \\phi_{obs} \\pm \\epsilon$ (with addition in the maximisation case).\n",
    "The value of $\\nu_Y$ can be any positive value, since both $\\phi$ and $\\sigma$ are independent of it.\n",
    "\n",
    "The corresponding Lagrangians are: \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_{\\min}(\\mu_X, \\nu_Y, \\lambda) =  \\mu_X\n",
    "+ \\lambda(|\\mu_X - \\phi_{obs}| - \\epsilon), \\\\\n",
    "\\mathcal{L}_{\\max}(\\mu_X, \\nu_Y, \\lambda) = - \\mu_X\n",
    "+ \\lambda(|\\mu_X - \\phi_{obs}| - \\epsilon)\n",
    "\\end{align*}\n",
    "\n",
    "where we require that $\\lambda\\geq 0$ in each case (to obtain a stationary point of the correct orientation).\n",
    "The KKT (primal-dual) solutions are $(\\mu_X^*, \\nu_Y, \\lambda^*) = (\\phi_{obs} \\pm \\epsilon, \\nu_Y, 1)$.\n",
    "\n",
    "In this notebook, with assistance from `causalprog`, we will attempt to find this solution using the naive approach of minimising  $\\| \\nabla \\mathcal{L} \\|_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Graph\n",
    "\n",
    "To setup a problem in `causalprog`, we need to construct the Directed Acyclic Graph (DAG) that represents the causal problem.\n",
    "The `Graph` class is how DAGs are represented in `causalprog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalprog.graph import Graph\n",
    "\n",
    "# Initialises the graph. Note that this graph currently doesn't\n",
    "# have any nodes or edges in it!\n",
    "graph = Graph(label=\"two_normal_graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Nodes\n",
    "\n",
    "For each parameter and random variable (RV) in our causal problem, we need a node to represent it.\n",
    "\n",
    "1. **Model Parameters** are represented with `ParameterNode`s. The model parameters are the set of variables that fully parametrise the (RVs that appear in the) DAG / causal problem.\n",
    "   - In our example, these are the values $\\mu_X$ and $\\nu_Y$.\n",
    "   - For each of these, we add a `ParameterNode`. \n",
    "   - Model parameters that are referenced in the `parameters` dictionary of a `DistributionNode` will be used when constructing the `DistributionNode`'s RV.\n",
    "\n",
    "2. **Derived (RV) Parameters** are parameters of RVs that are the result of sampling from a previous distribution, or take the value of a model parameter.\n",
    "   - In our example, the mean of $Y$ is such a quantity. This quantity is determined by the RV $X$.\n",
    "   - Also in our example, the mean of $X$ is such a quantity, being determined by $\\mu_X$ (a model parameter).\n",
    "   - Derived parameters should be provided to the `parameters` argument of a `DistributionNode`, and are stored in its eponymous attribute.\n",
    "\n",
    "1. **Constant (RV) Parameters** are constant values that appear in the problem, but are still needed to construct some of the RVs.\n",
    "   - In our example, the (co)variance of $X$ is a constant parameter, taking the value $1$.\n",
    "   - These values should be passed to the `constant_parameters` argument of a `DistributionNode`, and are stored in its eponymous attribute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.distributions import Normal\n",
    "\n",
    "from causalprog.graph import DistributionNode, ParameterNode\n",
    "\n",
    "graph.add_node(ParameterNode(label=\"mu_X\"))\n",
    "graph.add_node(\n",
    "    DistributionNode(\n",
    "        distribution=Normal,\n",
    "        label=\"X\",\n",
    "        constant_parameters={\"scale\": 1},  # Variance of X is constant\n",
    "        parameters={\"loc\": \"mu_X\"},  # Mean of X is given by mu_X\n",
    "    )\n",
    ")\n",
    "\n",
    "graph.add_node(ParameterNode(label=\"nu_Y\"))\n",
    "graph.add_node(\n",
    "    DistributionNode(\n",
    "        distribution=Normal,\n",
    "        label=\"Y\",\n",
    "        parameters={\n",
    "            \"loc\": \"X\",  # Mean of Y determined by the RV X\n",
    "            \"scale\": \"nu_Y\",  # Variance of Y is given by nu_Y\n",
    "        },\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Edges\n",
    "\n",
    "We must add edges between nodes in the graph to allow it to pick up connections between parameters and RVs, and derived parameters and other RVs.\n",
    "There is no need to add nodes for constant parameters (nor a need for edges to have these constants picked up by the nodes that use them).\n",
    "\n",
    "Note that edges should be directed **into** the dependent RV.\n",
    "That is, `DistributionNode`s should have edges directed into them from the nodes referenced by their derived parameters.\n",
    "These nodes may be either `ParameterNode`s; for example our RV $X$ will need an edge from the `ParameterNode` for $\\mu_X$ into the `DistributionNode` representing $X$.\n",
    "They may also be other `DistributionNode`s; for example, $Y$ has $X$ as a derived parameter since we know that $Y \\vert X \\sim \\mathcal{N}(X, \\nu_Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know Y | X, so the edge is directed X -> Y\n",
    "graph.add_edge(\"X\", \"Y\")\n",
    "\n",
    "# Parameters should point to the RVs they are used to define\n",
    "graph.add_edge(\"mu_X\", \"X\")\n",
    "graph.add_edge(\"nu_Y\", \"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Problem\n",
    "\n",
    "We can now use our graph to define the problem we wish to solve.\n",
    "To do so, we define our constraints using the `Constraint` class, and our causal estimand with the `CausalEstimand` class.  \n",
    "\n",
    "Each `Constraint` requires a function to calculate the constrained quantity, e.g. $\\mathbb{E}[X]$, from samples (from RVs) of the graph.\n",
    "In the function, we reference nodes with the labels we assigned previously (`\"X\"` and `\"Y\"`).  \n",
    "\n",
    "Likewise, each `CausalEstimand` requires a function to calculate the causal estimand, $\\mathbb{E}[Y]$, from samples (from RVs) of the graph.\n",
    "Again, we use the labels we assigned previously.\n",
    "\n",
    "We then pass our `Constraint`s and `CausalEstimand` to the `CausalProblem` class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalprog.causal_problem.causal_problem import CausalProblem\n",
    "from causalprog.causal_problem.components import CausalEstimand, Constraint\n",
    "\n",
    "# Define the constraint using observed data and a tolerance level.\n",
    "PHI_OBSERVED = 0.0\n",
    "EPSILON = 1\n",
    "constraint = Constraint(\n",
    "    model_quantity=lambda **pv: pv[\"X\"].mean(),\n",
    "    data=PHI_OBSERVED,\n",
    "    tolerance=EPSILON,\n",
    ")\n",
    "\n",
    "# Define the causal estimand\n",
    "causal_estimand = CausalEstimand(do_with_samples=lambda **pv: pv[\"Y\"].mean())\n",
    "\n",
    "# Define the problem using the graph, constraint, and causal estimand.\n",
    "causal_problem = CausalProblem(\n",
    "    graph,\n",
    "    constraint,\n",
    "    causal_estimand=causal_estimand,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Causal Problem\n",
    "\n",
    "Now we calculate the bounds of the causal estimand.\n",
    "To do this, we'll seek the stationary points of the Lagrangian, using the naive approach of minimising $\\| \\nabla \\mathcal{L} \\|_2^2$.\n",
    "\n",
    "`causalprog` provides a few out-of-the-box solvers that we can utilise, and the `CausalProblem` class can construct some helpful functions for us to use, like the Lagrangian $\\mathcal{L}$ for our problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy.typing as npt\n",
    "import optax\n",
    "\n",
    "from causalprog.solvers.sgd import stochastic_gradient_descent\n",
    "from causalprog.solvers.solver_callbacks import tqdm_callback\n",
    "from causalprog.solvers.solver_result import SolverResult\n",
    "from causalprog.utils.norms import l2_normsq\n",
    "\n",
    "# Define our initial guess for the decision variables and lagrange multiplier\n",
    "LAGRANGE_MULTIPLIER_INIT = jnp.atleast_1d(0.5)\n",
    "INIT_PARAMS = {\n",
    "    \"mu_X\": 0.0,\n",
    "    \"nu_Y\": 1.0,\n",
    "}\n",
    "\n",
    "# Choose stochastic gradient descent settings\n",
    "RNG_KEY = jax.random.key(42)\n",
    "LEARNING_RATE = 1.0e-1\n",
    "MAX_OPTIMISER_ITER = 200\n",
    "optimiser = optax.adam(LEARNING_RATE)\n",
    "\n",
    "\n",
    "# Define a function to find a bound using a naive Lagrangian approach\n",
    "def naive_lagrangian_approach(maximum_problem: bool) -> SolverResult:\n",
    "    # Have the CasualProblem class construct the Lagrangian for us.\n",
    "    # We can specify here whether we are seeking the maximum or minimum\n",
    "    # bound for the Causal Estimand.\n",
    "    lagrangian = causal_problem.lagrangian(\n",
    "        n_samples=300,\n",
    "        maximum_problem=maximum_problem,\n",
    "    )\n",
    "\n",
    "    # Define the objective function for optimization.\n",
    "    # Our naive approach requires us to minimise the L2-norm of the Lagrangian's\n",
    "    # gradient.\n",
    "    def objective(x: npt.ArrayLike, key: jax.Array) -> npt.ArrayLike:\n",
    "        v = jax.grad(lagrangian, argnums=(0, 1))(*x, rng_key=key)\n",
    "        return l2_normsq(v)\n",
    "\n",
    "    # Invoke the SDG solver to return the solution.\n",
    "    return stochastic_gradient_descent(\n",
    "        obj_fn=objective,\n",
    "        initial_guess=(INIT_PARAMS, LAGRANGE_MULTIPLIER_INIT),\n",
    "        fn_kwargs={\"key\": RNG_KEY},\n",
    "        maxiter=MAX_OPTIMISER_ITER,\n",
    "        optimiser=optimiser,\n",
    "        history_logging_interval=1,  # Log optimisation history every iteration\n",
    "        callbacks=tqdm_callback(MAX_OPTIMISER_ITER),  # Add a progress bar\n",
    "    )\n",
    "\n",
    "\n",
    "# Calculate the bounds\n",
    "max_result = naive_lagrangian_approach(maximum_problem=True)\n",
    "min_result = naive_lagrangian_approach(maximum_problem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "`causalprog`'s solvers, like `stochastic_gradient_descent`, return a `SolverResult` object.\n",
    "The `SolverResult` stores helpful information about the optimisation process.  \n",
    "\n",
    "The attributes of `SolverResult` that are most relevant are:\n",
    "\n",
    "- `fn_args`: The objective arguments at the final iteration. \n",
    "- `obj_val`: The objective value at `fn_args`.\n",
    "- `fn_args_history`: The history of `fn_args` at each iteration that the optimisation is logged. \n",
    "- `obj_val_history`: The history of `obj_val` at each iteration that the optimisation is logged. \n",
    "\n",
    "We can inspect these attributes to display the results of our optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from causalprog.solvers.solver_result import SolverResult\n",
    "\n",
    "\n",
    "# Since we want to print results for both the maximisation and minimisation problem,\n",
    "# we'll define a function to print the results of the SGD runs given the SolverResult\n",
    "def print_sgd_results(result: SolverResult, maximum_problem: bool = True) -> None:\n",
    "    \"\"\"Print the results of the stochastic gradient descent runs.\"\"\"\n",
    "    if maximum_problem:\n",
    "        print(\"-------------Max Bound Results-------------\")\n",
    "    else:\n",
    "        print(\"-------------Min Bound Results-------------\")\n",
    "    mindex = np.argmin(result.obj_val_history)\n",
    "    print(f\"Best Parameters: {result.fn_args_history[mindex][0]}\")\n",
    "    print(f\"Best Lagrange Multiplier: {result.fn_args_history[mindex][1][0]}\")\n",
    "    print(f\"Best Objective Value: {result.obj_val_history[mindex]}\\n\")\n",
    "\n",
    "\n",
    "print_sgd_results(max_result, maximum_problem=True)\n",
    "print_sgd_results(min_result, maximum_problem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the result we found for the minimum bound is incorrect. \n",
    "This is because both $(\\mu_X, \\lambda) = (1, -1)$ and $(\\mu_X, \\lambda) = (1, 1)$ provide us with $\\vert \\nabla \\mathcal{L}_{\\min} \\vert _2^2 = 0$.\n",
    "\n",
    "Clearly this is not ideal. \n",
    "To combat this, we could retry the search with multiple starting points, select the best one we find and hope it's optimal.\n",
    "\n",
    "Or we could try something else..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving In a Smarter Way\n",
    "\n",
    "`causalprog` provides us with an easy way to assemble functions that are relevant to the optimisation problem we need to solve.\n",
    "But we are free to further manipulate these functions to suit our particular problem.\n",
    "\n",
    "Given that our problem is convex, finite, and satisfies Slater's condition, we can use an objective function that targets the KKT conditions directly.  \n",
    "If these conditions are met, our solution will be optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "import optax\n",
    "\n",
    "\n",
    "def build_kkt_residual_obj(\n",
    "    cp: CausalProblem,\n",
    "    *,\n",
    "    n_samples: int = 1000,\n",
    "    maximum_problem: bool = False,\n",
    "    alpha: float = 1.0,  # weight: primal feasibility\n",
    "    beta: float = 1.0,  # weight: complementary slackness\n",
    "    gamma: float = 1.0,  # weight: dual feasibility\n",
    ") -> Callable[..., jax.Array]:\n",
    "    lagrangian = cp.lagrangian(n_samples=n_samples, maximum_problem=maximum_problem)\n",
    "\n",
    "    def obj(x: npt.ArrayLike, *, rng_key: jax.Array) -> jax.Array:\n",
    "        params, lam = x\n",
    "        grad_theta, g = jax.grad(lagrangian, argnums=(0, 1))(\n",
    "            params, lam, rng_key=rng_key\n",
    "        )\n",
    "\n",
    "        # 1) Stationarity\n",
    "        stat = l2_normsq(grad_theta)\n",
    "\n",
    "        # 2) Primal feasibility\n",
    "        primal = jnp.sum(jnp.maximum(g, 0.0) ** 2)\n",
    "\n",
    "        # 3) Complementary slackness\n",
    "        fb = jnp.sqrt(lam**2 + g**2) - lam + g  # Fischer-Burmeister\n",
    "        fb = jnp.sum(fb**2)\n",
    "\n",
    "        # 4) Dual feasibility\n",
    "        dual = jnp.sum(jnp.minimum(lam, 0.0) ** 2)\n",
    "\n",
    "        return stat + alpha * primal + beta * fb + gamma * dual\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "ALPHA = 1.0  # Primal feasibility\n",
    "BETA = 1.0  # Complementary slackness\n",
    "GAMMA = 1.0  # Dual feasibility\n",
    "\n",
    "optimiser = optax.adam(0.01)\n",
    "\n",
    "\n",
    "# Define a function to find a bound using the KKT residual approach\n",
    "def kkt_residual_approach(maximum_problem: bool) -> SolverResult:\n",
    "    objective = build_kkt_residual_obj(\n",
    "        causal_problem,\n",
    "        n_samples=200,\n",
    "        maximum_problem=maximum_problem,\n",
    "        alpha=ALPHA,\n",
    "        beta=BETA,\n",
    "        gamma=GAMMA,\n",
    "    )\n",
    "\n",
    "    return stochastic_gradient_descent(\n",
    "        obj_fn=objective,\n",
    "        initial_guess=(INIT_PARAMS, LAGRANGE_MULTIPLIER_INIT),\n",
    "        convergence_criteria=lambda x, _: jnp.abs(x),\n",
    "        fn_kwargs={\"rng_key\": RNG_KEY},\n",
    "        maxiter=1000,\n",
    "        tolerance=1e-5,  # We can set a tolerance for which convergence is successful\n",
    "        optimiser=optimiser,\n",
    "        history_logging_interval=1,\n",
    "        callbacks=[tqdm_callback(1000)],\n",
    "    )\n",
    "\n",
    "\n",
    "max_result = kkt_residual_approach(maximum_problem=True)\n",
    "min_result = kkt_residual_approach(maximum_problem=False)\n",
    "\n",
    "print_sgd_results(max_result, maximum_problem=True)\n",
    "print_sgd_results(min_result, maximum_problem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found (an approximation to) the correct bounds! ðŸŽ‰\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
